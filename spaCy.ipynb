{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d868dcf76e7299",
   "metadata": {},
   "source": [
    "### Simple One-liners\n",
    "\n",
    "<b>Python one-liners</b> are short programs that perform powerful operations, doing a lot within a single line of code.\n",
    "\n",
    "One-liners are very common in text processing, which is why we’re introducing a few examples here to help you better understand the code we are going to show today. Run the code cell below, can you explain what is happening in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf303936c8d9091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:24:25.078084Z",
     "start_time": "2025-11-19T21:24:25.075108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "# Simple for loop\n",
    "result = [x * 2 for x in range(5)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c9c4a5624eb8b",
   "metadata": {},
   "source": [
    "A list can also be created as a one-liner with conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e185c0b4ae49e538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:22:21.355650Z",
     "start_time": "2025-11-19T21:22:21.352981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'd', 'f']\n"
     ]
    }
   ],
   "source": [
    "letters = list(\"abCdEfG\")\n",
    "\n",
    "lower = []\n",
    "for letter in letters:\n",
    "    if letter.islower():\n",
    "        lower.append(letter)\n",
    "\n",
    "# The code above does the same thing as this one-liner:\n",
    "lower = [letter for letter in letters if letter.islower()]\n",
    "print(lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f2327a620eb4",
   "metadata": {},
   "source": [
    "One can even add transformations on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f2e86faf3ff003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:40:43.037650Z",
     "start_time": "2025-11-19T21:40:43.034683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'from', 'python']\n"
     ]
    }
   ],
   "source": [
    "words = \"Hello World from Python !\".split(\" \")\n",
    "lower = [w.lower() for w in words if w.isalpha()]\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562fd73a2004f0f",
   "metadata": {},
   "source": [
    "In general such one-liners follow the following template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bf080b434143e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[expression for item in iterable if condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161d17c0b709817",
   "metadata": {},
   "source": [
    "#### Exercise 1:  One-liner\n",
    "1. From a list of integers, write a one-liner that keeps only the even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3aeeede5e9dc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:36:05.857070Z",
     "start_time": "2025-11-19T21:36:05.850857Z"
    }
   },
   "outputs": [],
   "source": [
    "numbers = [3, 4, 7, 10, 11, 14]\n",
    "# even_number ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9593d042addd6",
   "metadata": {},
   "source": [
    "2. Take a list of words, convert each to uppercase, and concatenate them into one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0f37f2d9259097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:11:27.499114Z",
     "start_time": "2025-11-20T09:11:27.496622Z"
    }
   },
   "outputs": [],
   "source": [
    "words = [\"how\",\"was\",\"the\",\"mensa\",\"today\"]\n",
    "# sentence ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72f3b55ef0b338",
   "metadata": {},
   "source": [
    "---\n",
    "### spaCy\n",
    "#### Installation\n",
    "Before you install spaCy and its dependencies, make sure that your `pip`, `setuptools` and `wheel` are up to date by clicking the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f181563602febf17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:33:23.882070Z",
     "start_time": "2025-11-19T13:33:22.725239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.45.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install setuptools wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acd5c0065440ec",
   "metadata": {},
   "source": [
    "Then install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd28a027e02347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenna\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2871c644dd8927",
   "metadata": {},
   "source": [
    "Or you can do the steps above directly in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "985d01f03037d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U pip setuptools wheel\n",
    "#pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4416517dc9f15",
   "metadata": {},
   "source": [
    "After installing spaCy, you will also need to download a language model. Use the following cell to download a basic English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aa4fe7cadafcd88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:38:50.793648Z",
     "start_time": "2025-11-19T13:38:46.195343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5555402ffc22",
   "metadata": {},
   "source": [
    "Now let's create a new spaCy object using `spacy.load()`. What you put as the parameter here shall match with the model you downloaded earlier. If you want to try another model, you shall change the name as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681d92f7877e0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:46:45.927797Z",
     "start_time": "2025-11-19T13:46:44.305284Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25584daaf50bec38",
   "metadata": {},
   "source": [
    "After that we can use this spaCy object to parse a short piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e353895aa6c32aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:49:22.715701Z",
     "start_time": "2025-11-19T13:49:22.675516Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are taking here one paragraph from The Picture of Dorian Gray as an example\n",
    "text = \"Dorian Gray hurried along the quay through the drizzling rain. His meeting with Adrian Singleton had strangely moved him, and he wondered if the ruin of that young life was really to be laid at his door, as Basil Hallward had said to him with such infamy of insult. He bit his lip, and for a few seconds his eyes grew sad. Yet, after all, what did it matter to him? One's days were too brief to take the burden of another's errors on one's shoulders. Each man lived his own life, and paid his own price for living it. The only pity was one had to pay so often for a single fault. One had to pay over and over again, indeed. In her dealings with man Destiny never closed her accounts.\"\n",
    "\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb45b469ac9f6e6",
   "metadata": {},
   "source": [
    "---\n",
    "#### Tokenization and Word Counter\n",
    "\n",
    "By simply passing our text to spaCy, it is going to tokenize the text and give us the following basic information about the text:\n",
    "1. All of the sentences (doc.sents)\n",
    "1. All of the words (doc)\n",
    "1. All of the \"named entities,\": names of places, people, #brands, etc. (doc.ents)\n",
    "1. All none phrases or \"noun_chunks\": nouns in the text plus surrounding matter like adjectives and articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61749e3e18ca0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:02:40.656430Z",
     "start_time": "2025-11-19T21:02:40.653326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 9 sentences in the text.\n",
      "Sample sentences:\n",
      "Yet, after all, what did it matter to him?\n",
      "He bit his lip, and for a few seconds his eyes grew sad.\n",
      "His meeting with Adrian Singleton had strangely moved him, and he wondered if the ruin of that young life was really to be laid at his door, as Basil Hallward had said to him with such infamy of insult.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 1. sentences\n",
    "sentences = list(doc.sents)\n",
    "print(\"There are in total\", len(sentences), \"sentences in the text.\")\n",
    "\n",
    "print(\"Sample sentences:\")\n",
    "for item in random.sample(sentences, 3):\n",
    "    print(item.text.strip().replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688c54e0dcf651a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a686f685e70127e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T21:11:11.810222Z",
     "start_time": "2025-11-19T21:11:11.802929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dorian, Gray, hurried, along, the, quay, through, the, drizzling, rain, His, meeting, with, Adrian, Singleton, had, strangely, moved, him, and, he, wondered, if, the, ruin, of, that, young, life, was, really, to, be, laid, at, his, door, as, Basil, Hallward, had, said, to, him, with, such, infamy, of, insult, He, bit, his, lip, and, for, a, few, seconds, his, eyes, grew, sad, Yet, after, all, what, did, it, matter, to, him, One, days, were, too, brief, to, take, the, burden, of, another, errors, on, one, shoulders, Each, man, lived, his, own, life, and, paid, his, own, price, for, living, it, The, only, pity, was, one, had, to, pay, so, often, for, a, single, fault, One, had, to, pay, over, and, over, again, indeed, In, her, dealings, with, man, Destiny, never, closed, her, accounts]\n"
     ]
    }
   ],
   "source": [
    "# 2. words\n",
    "# tokens = [token for token in doc]\n",
    "words = [token for token in doc if token.is_alpha]\n",
    "#print(tokens)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e868957e2c47d",
   "metadata": {},
   "source": [
    "The list of words that we are composing here is actually a list of spaCy [Token](https://spacy.io/api/token) objects. To compose a list of words, we are using `.is_alpha` attribute from the Token class, which returns true if the token consist of alphabetic characters. In the code cell above, compare `tokens` and `words`, what's the difference?\n",
    "\n",
    "<b>Entities</b> are important in NLP because they usually contain information about the “who/what/where,” making them the most information-dense parts of a text. Identifying them helps us quickly understand the main topic. It also automatically group multi-word concepts for you. The process of extracting entities from text is called <b>Named Entity Recognition (NER)</b>. The NER label scheme varies by language and depends heavily on the training data available. You can find all available entity types for `en_core_web_sm` [here](https://spacy.io/models/en#en_core_web_sm-labels).\n",
    "\n",
    "`noun_chunks` are also very useful in this regard. They are commonly used for text summarization and keyword extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daccea73dc0ba8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:37:29.666421Z",
     "start_time": "2025-11-19T14:37:29.660256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorian Gray PERSON\n",
      "Adrian Singleton PERSON\n",
      "Basil Hallward PERSON\n",
      "a few seconds TIME\n",
      "One CARDINAL\n",
      "days DATE\n",
      "Noun phrases: ['Dorian Gray', 'the quay', 'the drizzling rain', 'His meeting', 'Adrian Singleton', 'him', 'he', 'the ruin', 'that young life', 'his door', 'Basil Hallward', 'him', 'such infamy', 'insult', 'He', 'his lip', 'a few seconds', 'his eyes', 'what', 'it', 'him', \"One's days\", 'the burden', \"another's errors\", \"one's shoulders\", 'Each man', 'his own life', 'his own price', 'it', 'The only pity', 'a single fault', 'her dealings', 'man Destiny', 'her accounts']\n"
     ]
    }
   ],
   "source": [
    "# 3. Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# 4. Noun phrases\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376893d8a20250b3",
   "metadata": {},
   "source": [
    "We can also visualise entities from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cdb7af3c20a681f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:46:59.269019Z",
     "start_time": "2025-11-20T08:46:59.253324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">&quot;As We May Think&quot; is an essay written by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vannevar Bush\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1945\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". He is an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " engineer.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "doc_demo = nlp('\"As We May Think\" is an essay written by Vannevar Bush in 1945. He is an American engineer.')\n",
    "displacy.render(doc_demo, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b35caed6636313",
   "metadata": {},
   "source": [
    "This is more useful if we load the whole text and try to take a look in it. This can take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c58785fe35bb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:41:11.902561Z",
     "start_time": "2025-11-19T15:40:55.989582Z"
    }
   },
   "outputs": [],
   "source": [
    "text = open(\"../week４/pg26740.txt\").read()\n",
    "full_doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851179e17884e7",
   "metadata": {},
   "source": [
    "And let's make a word counter and print the 10 most common words from the text. What do you expect the result to be like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7ced407682afccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:45:04.780011Z",
     "start_time": "2025-11-19T15:45:04.646892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3558), ('of', 2286), ('and', 2195), ('to', 2153), ('I', 1694), ('a', 1629), ('that', 1302), ('in', 1233), ('you', 1146), ('was', 1066)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words =  [token for token in full_doc if token.is_alpha]\n",
    "word_count = Counter([w.text for w in all_words])\n",
    "\n",
    "print(word_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8206797188d81f7",
   "metadata": {},
   "source": [
    "We see that it's not so useful to analyse these words as they don't represent the content of the text. These words are called <b>stop words</b> and there is a list for such words in spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af069600f9c1ae15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:57:18.262016Z",
     "start_time": "2025-11-19T15:57:18.258622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ever', 'myself', 'such', 'eight', 'thereupon', 'my', 'between', 'which', 'off', 'six', 'latterly', 'used', \"'ve\", 'eleven', 'say', 'one', 'became', 'few', 'several', 'towards', 'becomes', 'until', 'yourselves', 'empty', 'his', 'again', 'herein', 'indeed', 'hereupon', 'also', 'whereafter', 'almost', 'least', 'still', 'this', 'whatever', 'yourself', 'or', 'first', 'nothing', 'when', 'beyond', 'something', 'see', 'whoever', 'below', 'she', 'top', 'more', 'name', 'above', 'anyone', 'perhaps', 'sometime', 'your', 'once', 'within', 'toward', \"'s\", '’d', 'must', 'what', 'really', 'whether', 'am', 'of', 'many', 'noone', \"'re\", 'another', 'full', 'hereafter', 'where', 'latter', 'will', 'seem', 'former', 'either', 'all', 'sometimes', 'but', 'due', 'has', 'else', 'somehow', 'though', 'elsewhere', 'somewhere', 'a', 'on', 'always', 'thereafter', 'whom', 'regarding', 'whereupon', 'throughout', 'onto', 'before', 'everyone', 'both', 'yet', 'then', 'call', 'could', 'various', 'who', 'anyway', 'otherwise', 'yours', 'he', 'enough', 'made', 'because', 'further', 'becoming', 'doing', 'mostly', 'through', 'her', 'anyhow', 'part', 'are', '’m', 'cannot', 'some', 'at', 'per', 'any', 'back', 'fifteen', 'how', 'moreover', 'to', 'even', 'no', 'bottom', 'two', 'under', '‘m', 'those', 'does', 'anywhere', 'make', 'whereby', 'being', \"'d\", 'wherever', 'whole', 'most', 'go', 'nine', 'did', 'others', 'here', 'hers', 'every', 'their', 'that', '‘ve', 'get', 'seemed', 'thus', 'sixty', 'whence', 'whose', 'mine', 'nor', 're', 'anything', 'besides', 'in', 'they', 'everywhere', 'n’t', 'please', '‘s', 'down', 'too', 'should', 'front', 'hence', 'as', 'last', 'together', 'each', 'formerly', 'out', 'into', 'fifty', '‘re', 'seems', 'ca', 'keep', 'take', 'these', 'whenever', 'with', 'only', 'very', 'was', 'own', 'have', 'namely', 'show', 'had', 'four', 'across', 'thence', 'ours', 'we', 'and', 'about', 'same', \"'ll\", 'three', 'none', 'than', 'beforehand', 'seeming', 'them', 'the', 'ourselves', 'be', '’re', 'hundred', 'would', 'is', 'therefore', 'me', 'often', 'might', 'our', 'whereas', 'much', 'for', 'afterwards', 'himself', 'while', 'from', 'may', 'its', 'put', 'although', 'however', '’ll', 'whither', 'why', 'were', 'itself', 'beside', 'him', '’s', 'quite', 'everything', 'so', 'an', 'upon', 'by', 'without', 'someone', 'over', 'thru', 'using', 'amongst', 'done', 'thereby', 'do', 'wherein', 'twenty', 'been', 'third', 'therein', 'after', 'meanwhile', 'not', 'against', 'unless', 'since', '‘d', 'hereby', 'if', 'already', 'behind', 'it', \"n't\", 'next', 'up', 'nevertheless', 'us', 'rather', 'except', 'ten', 'during', 'can', 'you', 'other', 'twelve', 'neither', 'give', 'never', 'forty', 'via', 'alone', 'serious', '’ve', 'well', 'amount', 'side', 'now', '‘ll', 'become', 'among', 'i', 'themselves', 'nowhere', 'just', \"'m\", 'along', 'n‘t', 'herself', 'nobody', 'less', 'move', 'around', 'there', 'five'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceef0482168f6a2",
   "metadata": {},
   "source": [
    "We can then go ahead to remove these words from our word counter to see some more meaningful statistics from our word counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7655015e230837a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:45:03.076003Z",
     "start_time": "2025-11-20T07:45:03.030141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83533 33956\n",
      "[('Dorian', 417), ('said', 262), ('Lord', 245), ('Henry', 236), ('like', 221), ('life', 217), ('Gray', 204), ('man', 179), ('know', 175), ('Harry', 175), ('Basil', 158), ('things', 126), ('think', 126), ('thing', 121), ('eyes', 109), ('good', 107), ('come', 107), ('face', 106), ('want', 105), ('time', 103)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# create a list of words without stop words, also considering different cases\n",
    "all_words_without_sw = [word for word in all_words if word.text.lower() not in STOP_WORDS]\n",
    "print(len(all_words), len(all_words_without_sw))\n",
    "word_count = Counter([w.text for w in all_words_without_sw])\n",
    "print(word_count.most_common(20))\n",
    "most_common_adj = [adj for word in all_words_without_sw if word.pos_ == \"ADJECTIVE\" ]\n",
    "print(most_common_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf390f0a08993e",
   "metadata": {},
   "source": [
    "It is also possible to check if a token is part of stop words by using the `.is_stop` attribute from `Token` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621399e1c288889f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T20:48:58.536154Z",
     "start_time": "2025-11-19T20:48:58.533264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> Dorian False\n",
      "<class 'spacy.tokens.token.Token'> Gray False\n",
      "<class 'spacy.tokens.token.Token'> hurried False\n",
      "<class 'spacy.tokens.token.Token'> along True\n",
      "<class 'spacy.tokens.token.Token'> the True\n"
     ]
    }
   ],
   "source": [
    "sample = words[:5]\n",
    "for word in sample:\n",
    "    print(type(word), word.text, word.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac7be38ecab40f",
   "metadata": {},
   "source": [
    "Depending on the text you want to analyze, you can also customize the default stop-word list by adding or removing words as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3c664d90a91726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T20:58:20.804281Z",
     "start_time": "2025-11-19T20:58:20.801866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS.remove('again')\n",
    "print(\"again\" in STOP_WORDS)\n",
    "STOP_WORDS.add('again')\n",
    "print(\"again\" in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a16b36f42c73e6",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "- Load a text file of your choice\n",
    "- Create a word frequency counter\n",
    "- Improve the results by applying preprocessing steps (lowercasing, removing stop words, etc.)\n",
    "- Use spaCy to extract additional information from the text, such as named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21a61aba1021bebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 134\n",
      "[('language', 8), ('life', 4), ('think', 3), ('fully', 3), ('understand', 3), ('cultural', 3), ('technology', 2), ('interesting', 2), ('lot', 2), ('computational', 2), ('means', 2), ('AI', 2), ('individual', 2), ('actually', 2), ('mean', 2), ('meaning', 2), ('understood', 2), ('person', 2), ('understanding', 2), ('different', 2)]\n",
      "[]\n",
      "[('and', 17), ('the', 13), ('is', 13), ('that', 12), ('of', 11), ('language', 8), ('to', 8), ('our', 8), ('or', 7), ('what', 7)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "text = \"I think the topic of language and technology is very interesting, because even though there are a lot of approaches on how to process language through computational means such as LLMs and other technologies, I still think language is one of those domains that is still not completely compatible wiht tech and that technology or AI might never fully understand. Why is that? I think, because language is culturally and personally shaped. Everyone has is own language, every cultural group, every language and also every individual. To add to that language comes in many layers, while the computational devices can mostly cover what we see in front of our eyes or hear with our ears, a lot of whaT we actually mean is not the words we speak themselves or the hidden meaning that can only be understood through the cultural, individual context of the person saying and hearing. Language and communication is actually mostly based on the things we dont say or the unspoken and not what we write down or say out loud. Even though AI might come better at understanding us and can pretend to understand some cultural nuances, the very deeo and nuanced meaning of what I mean remains to the interpretation of the person that hears me and might be different for everyone. Communication is something that has no absolute answer or can be fully analyzed and understood like that but a very ambiguos and mysterious tool of our daily life, what might be the thing that makes our life interesting. Because if we already know and understand everything, what is our purpose to live? Why should we keep exploring and what whill keep touching our heart? Let's embrace the ambiguoty of language instead of analyzing it to a point where there is no coming back. Let's embrace not fully understanding our opposite and laughing together about our misunderstandings, that also means to sometimes be frustated or confused but that is what makes life life and us different from machines (at least up to this point). So that is how a discussion about la\"\n",
    "full_doc = nlp(text)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_words =  [token for token in full_doc if token.is_alpha]\n",
    "word_count = Counter([w.text for w in all_words])\n",
    "\n",
    "all_words_without_sw = [word for word in all_words if word.text.lower() not in STOP_WORDS]\n",
    "print(len(all_words), len(all_words_without_sw))\n",
    "word_count_without_stopwords = Counter([w.text for w in all_words_without_sw])\n",
    "print(word_count_without_stopwords.most_common(20))\n",
    "most_common_adj = [adj for word in all_words_without_sw if word.pos_ == \"ADJECTIVE\" ]\n",
    "print(most_common_adj)\n",
    "\n",
    "print(word_count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15672627479e2261",
   "metadata": {},
   "source": [
    "---\n",
    "#### POS(Parts of speech) Tagging\n",
    "After tokenization, spaCy can parse and tag a given Doc. POS tags provide information about a word in its context. spaCy provides such tagging in two systemsg: `.pos_` uses [universal POS tags](https://universaldependencies.org/u/pos/), while `.tag_` follows [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) system. The Penn Treebank POS tags are more detailed than the universal ones as it has different tags for verb in different tenses or noun is plural/singular forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55642a521039a359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:34:19.431791Z",
     "start_time": "2025-11-20T08:34:19.420753Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "doc_demo = nlp(\"The quick brown fox jumps over one of the lazy dogs.\")\n",
    "for token in doc_demo:\n",
    "    print(token.text, \"/\", token.pos_, \"/\", token.tag_)\n",
    "\n",
    "# Printing the sentence in tag\n",
    "demo_in_tag = \" \".join([token.tag_ for token in doc_demo])\n",
    "print(demo_in_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b7445feca1b77",
   "metadata": {},
   "source": [
    "Most of the tags and labels look pretty abstract, and they vary between languages. You can use `spacy.explain()` to show you a short description of what a tag means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e646731492c35b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:57:46.474149Z",
     "start_time": "2025-11-20T07:57:46.471144Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"VBZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43483e1bad80cf",
   "metadata": {},
   "source": [
    "We can use `pos_` or `tag_` to filter a particular type of words that we want from a text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c6de0e9f8a735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:59:26.875033Z",
     "start_time": "2025-11-20T07:59:26.871614Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Nouns:\", [token.text for token in doc if token.pos_ == \"NOUN\"])\n",
    "print(\"Verbs in past tense:\", [token.text for token in doc if token.tag_ == \"VBD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a007f096a3ce9",
   "metadata": {},
   "source": [
    "---\n",
    "#### Lemmatization and Inflection\n",
    "When we are compling a list of words from a text corpus, sometimes it makes sense to save the words in their most basic form. For that we will need the lemma of a word, or the process of lemmatization.\n",
    "\n",
    "A word's \"lemma\" is its most \"basic\" form, the form without any morphology applied to it. In the example above we have the word \"moved\", the past tense of \"move\", or \"seconds\", the plural form of \"second\".\n",
    "\n",
    "For example, we can get all the verbs, nouns, adjectives and adverbs without morphology from the example text with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360485309f624df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T07:47:20.998117Z",
     "start_time": "2025-11-20T07:47:20.995110Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print(\"Nouns:\", [token.lemma_ for token in doc if token.pos_ == \"NOUN\"])\n",
    "print(\"Adjectives:\", [token.lemma_ for token in doc if token.pos_ == \"ADJ\"])\n",
    "print(\"Adverbs:\", [token.lemma_ for token in doc if token.pos_ == \"ADV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b099d6e0c410d",
   "metadata": {},
   "source": [
    "The opposite process of lemmatization is called <b>Inflection</b>, when we want to change the form of a verb/noun according to its current context (like number, tense, case...).\n",
    "\n",
    "If we want to make sure that the words in a generated sentence is grammatically correct no matter what words got randomly chosen from a list, we could use a python library called `LemmInflect`. The system acts as a standalone module or as an extension to spaCy and it only works with English words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:06:08.088522Z",
     "start_time": "2025-11-19T16:06:07.566903Z"
    }
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install LemmInflect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a39e7729ba5c",
   "metadata": {},
   "source": [
    "Here is a demo of some commonly used transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782edd7f04b2745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:18:19.970925Z",
     "start_time": "2025-11-19T15:18:19.966271Z"
    }
   },
   "outputs": [],
   "source": [
    "from lemminflect import getInflection\n",
    "\n",
    "# Verb Demo\n",
    "print(getInflection('be', tag='VBD'))\n",
    "print(getInflection('be', tag='VBG'))\n",
    "print(getInflection('be', tag='VBN'))\n",
    "print(getInflection('be', tag='VBP'))\n",
    "print(getInflection('be', tag='VBZ'))\n",
    "\n",
    "# Noun Demo\n",
    "print(getInflection('tooth', tag='NNS'))\n",
    "print(getInflection('medium', tag='NNS'))\n",
    "\n",
    "# Noun Demo\n",
    "print(getInflection('good', tag='JJR'))\n",
    "print(getInflection('good', tag='JJS'))\n",
    "\n",
    "# pos_type = 'A'\n",
    "# * JJ      Adjective\n",
    "# * JJR     Adjective, comparative\n",
    "# * JJS     Adjective, superlative\n",
    "# * RB      Adverb\n",
    "# * RBR     Adverb, comparative\n",
    "# * RBS     Adverb, superlative\n",
    "#\n",
    "# pos_type = 'N'\n",
    "# * NN      Noun, singular or mass\n",
    "# * NNS     Noun, plural\n",
    "#\n",
    "# pos_type = 'V'\n",
    "# * VB      Verb, base form\n",
    "# * VBD     Verb, past tense\n",
    "# * VBG     Verb, gerund or present participle\n",
    "# * VBN     Verb, past participle\n",
    "# * VBP     Verb, non-3rd person singular present\n",
    "# * VBZ     Verb, 3rd person singular present\n",
    "# * MD      Modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e5a4baa416137",
   "metadata": {},
   "source": [
    "---\n",
    "### Sentence Level Analysis\n",
    "\n",
    "To understand how words are connected within a sentence, spaCy uses [Dependency grammar](https://en.wikipedia.org/wiki/Dependency_grammar) as its framework to process the text. In this framework, each word depends on a “head” word their dependency relation can be categorized into pre-defined types. It helps spaCy understand sentence structure so it can figure out things like subjects, objects, and how different parts of the sentence relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057094e13fe029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:47:37.961158Z",
     "start_time": "2025-11-20T08:47:37.950619Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_demo = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in st]).strip()\n",
    "\n",
    "for token in doc_demo:\n",
    "    print()\n",
    "    print(\"Word:\", token.text)\n",
    "    print(\"Tag:\", token.tag_)\n",
    "    print(\"Head:\", token.head.text)\n",
    "    print(\"Dependency relation:\", token.dep_)\n",
    "    print(\"Subtree:\", flatten_subtree(token.subtree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b31c342dc9f6ba",
   "metadata": {},
   "source": [
    "Spacy has its own visualizer and we can use it to help us understand what's happening here in an easier way. In the visualisation we can see that \"The quick brown fox\" are grouped together under the head \"fox\" because it is the center of this sequence. Same happens with \"the lazy dog\". It is in this way how spaCy picks up the noun chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f975ff6a1c57ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T12:55:14.584405Z",
     "start_time": "2025-11-20T12:55:14.560653Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc_demo, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21b41fc3f2770b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:32:36.518182Z",
     "start_time": "2025-11-20T08:32:36.512214Z"
    }
   },
   "outputs": [],
   "source": [
    "[chunk for chunk in doc_demo.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81069441365cc6f1",
   "metadata": {},
   "source": [
    "Let's see some concrete examples how this can be useful.  For example, we can get a whole phrase by its head using `Token.subtree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2fc3208dd6a31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T08:56:20.044881Z",
     "start_time": "2025-11-20T08:56:19.980180Z"
    }
   },
   "outputs": [],
   "source": [
    "prep_phrases = [] # prepositional phrase\n",
    "for token in full_doc:\n",
    "    if token.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(token.subtree).replace(\"\\n\", \" \")) # replace line break with space\n",
    "\n",
    "random.sample(prep_phrases, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7fb9e0316b484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:20:33.694110Z",
     "start_time": "2025-11-20T09:20:33.636035Z"
    }
   },
   "outputs": [],
   "source": [
    "# compose list of phrases with exact text matching\n",
    "\n",
    "def phrases_with_word(text):\n",
    "    return [flatten_subtree(token.subtree).replace(\"\\n\", \" \") for token in full_doc if token.text == text]\n",
    "\n",
    "# random.sample(phrases_with('with'),10)\n",
    "random.sample(phrases_with_word('wish'),10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b052d1fa89a04a",
   "metadata": {},
   "source": [
    "We are covering only the basic usages of spaCy today. For more tutorials or documentations, you can check out [the official website](https://spacy.io/usage/spacy-101). We will learn more about it in the later sessions of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cba17f8708cce",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "- Extract groups of words from your text. For example: lists of nouns, verbs, adjectives...\n",
    "- Try to do that also with longer phrases. Look closely at your text, what phrases might be interesting to extract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a9c769607c4ab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I / PRON / PRP\n",
      "think / VERB / VBP\n",
      "the / DET / DT\n",
      "topic / NOUN / NN\n",
      "of / ADP / IN\n",
      "language / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "technology / NOUN / NN\n",
      "is / AUX / VBZ\n",
      "very / ADV / RB\n",
      "interesting / ADJ / JJ\n",
      ", / PUNCT / ,\n",
      "because / SCONJ / IN\n",
      "even / ADV / RB\n",
      "though / SCONJ / IN\n",
      "there / PRON / EX\n",
      "are / VERB / VBP\n",
      "a / DET / DT\n",
      "lot / NOUN / NN\n",
      "of / ADP / IN\n",
      "approaches / NOUN / NNS\n",
      "on / ADP / IN\n",
      "how / SCONJ / WRB\n",
      "to / PART / TO\n",
      "process / VERB / VB\n",
      "language / NOUN / NN\n",
      "through / ADP / IN\n",
      "computational / ADJ / JJ\n",
      "means / NOUN / NNS\n",
      "such / ADJ / JJ\n",
      "as / ADP / IN\n",
      "LLMs / NOUN / NNS\n",
      "and / CCONJ / CC\n",
      "other / ADJ / JJ\n",
      "technologies / NOUN / NNS\n",
      ", / PUNCT / ,\n",
      "I / PRON / PRP\n",
      "still / ADV / RB\n",
      "think / VERB / VBP\n",
      "language / NOUN / NN\n",
      "is / AUX / VBZ\n",
      "one / NUM / CD\n",
      "of / ADP / IN\n",
      "those / DET / DT\n",
      "domains / NOUN / NNS\n",
      "that / PRON / WDT\n",
      "is / AUX / VBZ\n",
      "still / ADV / RB\n",
      "not / PART / RB\n",
      "completely / ADV / RB\n",
      "compatible / ADJ / JJ\n",
      "wiht / ADJ / JJ\n",
      "tech / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "that / DET / DT\n",
      "technology / NOUN / NN\n",
      "or / CCONJ / CC\n",
      "AI / PROPN / NNP\n",
      "might / AUX / MD\n",
      "never / ADV / RB\n",
      "fully / ADV / RB\n",
      "understand / VERB / VB\n",
      ". / PUNCT / .\n",
      "Why / SCONJ / WRB\n",
      "is / AUX / VBZ\n",
      "that / PRON / DT\n",
      "? / PUNCT / .\n",
      "I / PRON / PRP\n",
      "think / VERB / VBP\n",
      ", / PUNCT / ,\n",
      "because / SCONJ / IN\n",
      "language / NOUN / NN\n",
      "is / AUX / VBZ\n",
      "culturally / ADV / RB\n",
      "and / CCONJ / CC\n",
      "personally / ADV / RB\n",
      "shaped / ADJ / JJ\n",
      ". / PUNCT / .\n",
      "Everyone / PRON / NN\n",
      "has / AUX / VBZ\n",
      "is / AUX / VBZ\n",
      "own / ADJ / JJ\n",
      "language / NOUN / NN\n",
      ", / PUNCT / ,\n",
      "every / DET / DT\n",
      "cultural / ADJ / JJ\n",
      "group / NOUN / NN\n",
      ", / PUNCT / ,\n",
      "every / DET / DT\n",
      "language / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "also / ADV / RB\n",
      "every / DET / DT\n",
      "individual / NOUN / NN\n",
      ". / PUNCT / .\n",
      "To / PART / TO\n",
      "add / VERB / VB\n",
      "to / ADP / IN\n",
      "that / DET / DT\n",
      "language / NOUN / NN\n",
      "comes / VERB / VBZ\n",
      "in / ADP / IN\n",
      "many / ADJ / JJ\n",
      "layers / NOUN / NNS\n",
      ", / PUNCT / ,\n",
      "while / SCONJ / IN\n",
      "the / DET / DT\n",
      "computational / ADJ / JJ\n",
      "devices / NOUN / NNS\n",
      "can / AUX / MD\n",
      "mostly / ADV / RB\n",
      "cover / VERB / VB\n",
      "what / PRON / WP\n",
      "we / PRON / PRP\n",
      "see / VERB / VBP\n",
      "in / ADP / IN\n",
      "front / NOUN / NN\n",
      "of / ADP / IN\n",
      "our / PRON / PRP$\n",
      "eyes / NOUN / NNS\n",
      "or / CCONJ / CC\n",
      "hear / VERB / VBP\n",
      "with / ADP / IN\n",
      "our / PRON / PRP$\n",
      "ears / NOUN / NNS\n",
      ", / PUNCT / ,\n",
      "a / DET / DT\n",
      "lot / NOUN / NN\n",
      "of / ADP / IN\n",
      "whaT / PRON / WP\n",
      "we / PRON / PRP\n",
      "actually / ADV / RB\n",
      "mean / VERB / VBP\n",
      "is / AUX / VBZ\n",
      "not / PART / RB\n",
      "the / DET / DT\n",
      "words / NOUN / NNS\n",
      "we / PRON / PRP\n",
      "speak / VERB / VBP\n",
      "themselves / PRON / PRP\n",
      "or / CCONJ / CC\n",
      "the / DET / DT\n",
      "hidden / ADJ / JJ\n",
      "meaning / NOUN / NN\n",
      "that / PRON / WDT\n",
      "can / AUX / MD\n",
      "only / ADV / RB\n",
      "be / AUX / VB\n",
      "understood / VERB / VBN\n",
      "through / ADP / IN\n",
      "the / DET / DT\n",
      "cultural / ADJ / JJ\n",
      ", / PUNCT / ,\n",
      "individual / ADJ / JJ\n",
      "context / NOUN / NN\n",
      "of / ADP / IN\n",
      "the / DET / DT\n",
      "person / NOUN / NN\n",
      "saying / VERB / VBG\n",
      "and / CCONJ / CC\n",
      "hearing / NOUN / NN\n",
      ". / PUNCT / .\n",
      "Language / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "communication / NOUN / NN\n",
      "is / AUX / VBZ\n",
      "actually / ADV / RB\n",
      "mostly / ADV / RB\n",
      "based / VERB / VBN\n",
      "on / ADP / IN\n",
      "the / DET / DT\n",
      "things / NOUN / NNS\n",
      "we / PRON / PRP\n",
      "do / AUX / VBP\n",
      "nt / PART / RB\n",
      "say / VERB / VB\n",
      "or / CCONJ / CC\n",
      "the / DET / DT\n",
      "unspoken / ADJ / JJ\n",
      "and / CCONJ / CC\n",
      "not / PART / RB\n",
      "what / PRON / WP\n",
      "we / PRON / PRP\n",
      "write / VERB / VBP\n",
      "down / ADP / RP\n",
      "or / CCONJ / CC\n",
      "say / VERB / VB\n",
      "out / ADV / RB\n",
      "loud / ADV / RB\n",
      ". / PUNCT / .\n",
      "Even / ADV / RB\n",
      "though / SCONJ / IN\n",
      "AI / PROPN / NNP\n",
      "might / AUX / MD\n",
      "come / VERB / VB\n",
      "better / ADV / RBR\n",
      "at / ADP / IN\n",
      "understanding / VERB / VBG\n",
      "us / PRON / PRP\n",
      "and / CCONJ / CC\n",
      "can / AUX / MD\n",
      "pretend / VERB / VB\n",
      "to / PART / TO\n",
      "understand / VERB / VB\n",
      "some / DET / DT\n",
      "cultural / ADJ / JJ\n",
      "nuances / NOUN / NNS\n",
      ", / PUNCT / ,\n",
      "the / DET / DT\n",
      "very / ADV / RB\n",
      "deeo / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "nuanced / ADJ / JJ\n",
      "meaning / NOUN / NN\n",
      "of / ADP / IN\n",
      "what / PRON / WP\n",
      "I / PRON / PRP\n",
      "mean / VERB / VBP\n",
      "remains / VERB / VBZ\n",
      "to / ADP / IN\n",
      "the / DET / DT\n",
      "interpretation / NOUN / NN\n",
      "of / ADP / IN\n",
      "the / DET / DT\n",
      "person / NOUN / NN\n",
      "that / PRON / WDT\n",
      "hears / VERB / VBZ\n",
      "me / PRON / PRP\n",
      "and / CCONJ / CC\n",
      "might / AUX / MD\n",
      "be / AUX / VB\n",
      "different / ADJ / JJ\n",
      "for / ADP / IN\n",
      "everyone / PRON / NN\n",
      ". / PUNCT / .\n",
      "Communication / NOUN / NN\n",
      "is / AUX / VBZ\n",
      "something / PRON / NN\n",
      "that / PRON / WDT\n",
      "has / VERB / VBZ\n",
      "no / DET / DT\n",
      "absolute / ADJ / JJ\n",
      "answer / NOUN / NN\n",
      "or / CCONJ / CC\n",
      "can / AUX / MD\n",
      "be / AUX / VB\n",
      "fully / ADV / RB\n",
      "analyzed / VERB / VBN\n",
      "and / CCONJ / CC\n",
      "understood / VERB / VBD\n",
      "like / ADP / IN\n",
      "that / PRON / DT\n",
      "but / CCONJ / CC\n",
      "a / DET / DT\n",
      "very / ADV / RB\n",
      "ambiguos / ADJ / JJ\n",
      "and / CCONJ / CC\n",
      "mysterious / ADJ / JJ\n",
      "tool / NOUN / NN\n",
      "of / ADP / IN\n",
      "our / PRON / PRP$\n",
      "daily / ADJ / JJ\n",
      "life / NOUN / NN\n",
      ", / PUNCT / ,\n",
      "what / PRON / WP\n",
      "might / AUX / MD\n",
      "be / AUX / VB\n",
      "the / DET / DT\n",
      "thing / NOUN / NN\n",
      "that / PRON / WDT\n",
      "makes / VERB / VBZ\n",
      "our / PRON / PRP$\n",
      "life / NOUN / NN\n",
      "interesting / ADJ / JJ\n",
      ". / PUNCT / .\n",
      "Because / SCONJ / IN\n",
      "if / SCONJ / IN\n",
      "we / PRON / PRP\n",
      "already / ADV / RB\n",
      "know / VERB / VBP\n",
      "and / CCONJ / CC\n",
      "understand / VERB / VB\n",
      "everything / PRON / NN\n",
      ", / PUNCT / ,\n",
      "what / PRON / WP\n",
      "is / AUX / VBZ\n",
      "our / PRON / PRP$\n",
      "purpose / NOUN / NN\n",
      "to / PART / TO\n",
      "live / VERB / VB\n",
      "? / PUNCT / .\n",
      "Why / SCONJ / WRB\n",
      "should / AUX / MD\n",
      "we / PRON / PRP\n",
      "keep / VERB / VB\n",
      "exploring / VERB / VBG\n",
      "and / CCONJ / CC\n",
      "what / PRON / WP\n",
      "whill / AUX / MD\n",
      "keep / VERB / VB\n",
      "touching / VERB / VBG\n",
      "our / PRON / PRP$\n",
      "heart / NOUN / NN\n",
      "? / PUNCT / .\n",
      "Let / VERB / VB\n",
      "'s / PRON / PRP\n",
      "embrace / VERB / VB\n",
      "the / DET / DT\n",
      "ambiguoty / NOUN / NN\n",
      "of / ADP / IN\n",
      "language / NOUN / NN\n",
      "instead / ADV / RB\n",
      "of / ADP / IN\n",
      "analyzing / VERB / VBG\n",
      "it / PRON / PRP\n",
      "to / ADP / IN\n",
      "a / DET / DT\n",
      "point / NOUN / NN\n",
      "where / SCONJ / WRB\n",
      "there / PRON / EX\n",
      "is / VERB / VBZ\n",
      "no / DET / DT\n",
      "coming / VERB / VBG\n",
      "back / ADV / RB\n",
      ". / PUNCT / .\n",
      "Let / VERB / VB\n",
      "'s / PRON / PRP\n",
      "embrace / VERB / VB\n",
      "not / PART / RB\n",
      "fully / ADV / RB\n",
      "understanding / VERB / VBG\n",
      "our / PRON / PRP$\n",
      "opposite / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "laughing / VERB / VBG\n",
      "together / ADV / RB\n",
      "about / ADP / IN\n",
      "our / PRON / PRP$\n",
      "misunderstandings / NOUN / NNS\n",
      ", / PUNCT / ,\n",
      "that / PRON / WDT\n",
      "also / ADV / RB\n",
      "means / VERB / VBZ\n",
      "to / PART / TO\n",
      "sometimes / ADV / RB\n",
      "be / AUX / VB\n",
      "frustated / ADJ / JJ\n",
      "or / CCONJ / CC\n",
      "confused / ADJ / JJ\n",
      "but / CCONJ / CC\n",
      "that / PRON / DT\n",
      "is / AUX / VBZ\n",
      "what / PRON / WP\n",
      "makes / VERB / VBZ\n",
      "life / NOUN / NN\n",
      "life / NOUN / NN\n",
      "and / CCONJ / CC\n",
      "us / PRON / PRP\n",
      "different / ADJ / JJ\n",
      "from / ADP / IN\n",
      "machines / NOUN / NNS\n",
      "( / PUNCT / -LRB-\n",
      "at / ADP / IN\n",
      "least / ADJ / JJS\n",
      "up / ADP / IN\n",
      "to / ADP / IN\n",
      "this / DET / DT\n",
      "point / NOUN / NN\n",
      ") / PUNCT / -RRB-\n",
      ". / PUNCT / .\n",
      "So / ADV / RB\n",
      "that / PRON / DT\n",
      "is / AUX / VBZ\n",
      "how / SCONJ / WRB\n",
      "a / DET / DT\n",
      "discussion / NOUN / NN\n",
      "about / ADP / IN\n",
      "la / NOUN / NN\n",
      "PRP VBP DT NN IN NN CC NN VBZ RB JJ , IN RB IN EX VBP DT NN IN NNS IN WRB TO VB NN IN JJ NNS JJ IN NNS CC JJ NNS , PRP RB VBP NN VBZ CD IN DT NNS WDT VBZ RB RB RB JJ JJ NN CC DT NN CC NNP MD RB RB VB . WRB VBZ DT . PRP VBP , IN NN VBZ RB CC RB JJ . NN VBZ VBZ JJ NN , DT JJ NN , DT NN CC RB DT NN . TO VB IN DT NN VBZ IN JJ NNS , IN DT JJ NNS MD RB VB WP PRP VBP IN NN IN PRP$ NNS CC VBP IN PRP$ NNS , DT NN IN WP PRP RB VBP VBZ RB DT NNS PRP VBP PRP CC DT JJ NN WDT MD RB VB VBN IN DT JJ , JJ NN IN DT NN VBG CC NN . NN CC NN VBZ RB RB VBN IN DT NNS PRP VBP RB VB CC DT JJ CC RB WP PRP VBP RP CC VB RB RB . RB IN NNP MD VB RBR IN VBG PRP CC MD VB TO VB DT JJ NNS , DT RB NN CC JJ NN IN WP PRP VBP VBZ IN DT NN IN DT NN WDT VBZ PRP CC MD VB JJ IN NN . NN VBZ NN WDT VBZ DT JJ NN CC MD VB RB VBN CC VBD IN DT CC DT RB JJ CC JJ NN IN PRP$ JJ NN , WP MD VB DT NN WDT VBZ PRP$ NN JJ . IN IN PRP RB VBP CC VB NN , WP VBZ PRP$ NN TO VB . WRB MD PRP VB VBG CC WP MD VB VBG PRP$ NN . VB PRP VB DT NN IN NN RB IN VBG PRP IN DT NN WRB EX VBZ DT VBG RB . VB PRP VB RB RB VBG PRP$ NN CC VBG RB IN PRP$ NNS , WDT RB VBZ TO RB VB JJ CC JJ CC DT VBZ WP VBZ NN NN CC PRP JJ IN NNS -LRB- IN JJS IN IN DT NN -RRB- . RB DT VBZ WRB DT NN IN NN\n",
      "Adjectives: ['interesting', 'computational', 'such', 'other', 'compatible', 'wiht', 'shaped', 'own', 'cultural', 'many', 'computational', 'hidden', 'cultural', 'individual', 'unspoken', 'cultural', 'nuanced', 'different', 'absolute', 'ambiguos', 'mysterious', 'daily', 'interesting', 'frustated', 'confused', 'different', 'least']\n"
     ]
    }
   ],
   "source": [
    "text = \"I think the topic of language and technology is very interesting, because even though there are a lot of approaches on how to process language through computational means such as LLMs and other technologies, I still think language is one of those domains that is still not completely compatible wiht tech and that technology or AI might never fully understand. Why is that? I think, because language is culturally and personally shaped. Everyone has is own language, every cultural group, every language and also every individual. To add to that language comes in many layers, while the computational devices can mostly cover what we see in front of our eyes or hear with our ears, a lot of whaT we actually mean is not the words we speak themselves or the hidden meaning that can only be understood through the cultural, individual context of the person saying and hearing. Language and communication is actually mostly based on the things we dont say or the unspoken and not what we write down or say out loud. Even though AI might come better at understanding us and can pretend to understand some cultural nuances, the very deeo and nuanced meaning of what I mean remains to the interpretation of the person that hears me and might be different for everyone. Communication is something that has no absolute answer or can be fully analyzed and understood like that but a very ambiguos and mysterious tool of our daily life, what might be the thing that makes our life interesting. Because if we already know and understand everything, what is our purpose to live? Why should we keep exploring and what whill keep touching our heart? Let's embrace the ambiguoty of language instead of analyzing it to a point where there is no coming back. Let's embrace not fully understanding our opposite and laughing together about our misunderstandings, that also means to sometimes be frustated or confused but that is what makes life life and us different from machines (at least up to this point). So that is how a discussion about la\"\n",
    "full_doc = nlp(text)\n",
    "\n",
    "for token in full_doc:\n",
    "    print(token.text, \"/\", token.pos_, \"/\", token.tag_)\n",
    "\n",
    "demo_in_tag = \" \".join([token.tag_ for token in full_doc])\n",
    "print(demo_in_tag)\n",
    "print(\"Adjectives:\", [token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93383ace03ce6948",
   "metadata": {},
   "source": [
    "---\n",
    "### Assignment 2\n",
    "Option 1\n",
    "1. Find a text corpus that interests you\n",
    "2. Use spaCy to process and harvest groups of words/phrases\n",
    "3. Then use `tracery`to build a text generator based on the harvested material\n",
    "\n",
    "Option 2\n",
    "1. Find a text corpus that interests you\n",
    "2. Use spaCy to tag the text\n",
    "3. Use `Markovify`to generate new sentences based on tags\n",
    "4. Replace the tag with other words (either from same text or other resources)\n",
    "\n",
    "Or feel free to come up with your own approach, as long as you use spaCy to help you create a text generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e86465d9-d3c0-4a1c-b42c-31e8367229ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences: 1038\n",
      "\n",
      "Sample sentences:\n",
      "  - 孤鐙挑盡未成眠。\n",
      "  - 上有靑㝠之長天，下有綠水之波瀾。\n",
      "  - 憶君迢迢隔靑天，昔時橫波目，今作流淚泉。\n",
      "\n",
      "Total words/characters extracted: 6500\n",
      "\n",
      "Sample words:\n",
      "['蘭', '葉春', '葳蕤', '桂華', '秋皎潔', '欣欣', '此', '生意', '自爾', '為', '佳節', '誰知', '林棲者', '聞風', '坐', '相悅', '草木', '有', '本心', '何求']\n",
      "\n",
      "==================================================\n",
      "WORD FREQUENCY ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Most common 20 words:\n",
      "不: 90\n",
      "之: 52\n",
      "一: 46\n",
      "有: 43\n",
      "句: 39\n",
      "我: 35\n",
      "在: 33\n",
      "四: 29\n",
      "此: 27\n",
      "去: 26\n",
      "上: 24\n",
      "未: 22\n",
      "見: 22\n",
      "如: 21\n",
      "已: 20\n",
      "下: 19\n",
      "是: 19\n",
      "人: 18\n",
      "中: 18\n",
      "得: 17\n",
      "\n",
      "==================================================\n",
      "PART OF SPEECH ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Nouns found (2483 total):\n",
      "[('人', 18), ('君', 13), ('無', 12), ('月', 9), ('酒', 8), ('將軍', 8), ('人生', 6), ('今', 6), ('字', 5), ('夜', 5), ('心', 5), ('事', 5), ('曲', 5), ('萬', 5), ('古來', 5)]\n",
      "\n",
      "Verbs found (2159 total):\n",
      "[('有', 43), ('去', 26), ('是', 19), ('見', 17), ('能', 14), ('到', 11), ('為', 9), ('美人', 8), ('上', 8), ('至', 8), ('在', 8), ('得', 7), ('如', 7), ('使', 7), ('誰', 7)]\n",
      "\n",
      "Adjectives found (150 total):\n",
      "[('大', 9), ('新', 8), ('以下', 4), ('深松', 2), ('窮', 2), ('高', 2), ('小', 2), ('白', 2), ('萬里', 2), ('風', 2), ('自從', 2), ('歲', 1), ('奈何', 1), ('樹', 1), ('舉杯', 1)]\n",
      "\n",
      "Proper nouns found (407 total):\n",
      "[('中', 5), ('靑', 4), ('朝', 4), ('秦', 4), ('江南', 2), ('萬', 2), ('何處尋', 2), ('日', 2), ('楊', 2), ('餘', 2)]\n",
      "\n",
      "==================================================\n",
      "NAMED ENTITIES\n",
      "==================================================\n",
      "\n",
      "Named entities found in the poems:\n",
      "  林棲者 (PERSON)\n",
      "  江南 (LOC)\n",
      "  循環 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  三 (CARDINAL)\n",
      "  永結無情遊 (LOC)\n",
      "  春風 (DATE)\n",
      "  陰 (PERSON)\n",
      "  焉知 (PERSON)\n",
      "  二十 (CARDINAL)\n",
      "  剪春韭 (GPE)\n",
      "  十觴 (CARDINAL)\n",
      "  佳人 (GPE)\n",
      "  零落依 (CARDINAL)\n",
      "  日暮倚 (ORG)\n",
      "  江南 (LOC)\n",
      "  三夜 (TIME)\n",
      "  六 (CARDINAL)\n",
      "  斯人 (GPE)\n",
      "  六 (CARDINAL)\n",
      "  南山陲 (GPE)\n",
      "  莫復問 (PERSON)\n",
      "  白雲無 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  江淮渡 (GPE)\n",
      "  四 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  蠶眠桑 (GPE)\n",
      "  田夫 (PERSON)\n",
      "  北山 (GPE)\n",
      "  薄暮起 (PERSON)\n",
      "  江畔洲 (PERSON)\n",
      "  何當載 (PERSON)\n",
      "  西落 (LOC)\n",
      "  南亭 (GPE)\n",
      "  風泉 (PERSON)\n",
      "  開帷月 (PERSON)\n",
      "  四月 (DATE)\n",
      "  千 (CARDINAL)\n",
      "  三十 (CARDINAL)\n",
      "  新雨 (ORG)\n",
      "  春際 (DATE)\n",
      "  淸光猶 (PERSON)\n",
      "  西山鸞 (LOC)\n",
      "  四 (CARDINAL)\n",
      "  四角礙白日 (DATE)\n",
      "  二 (CARDINAL)\n",
      "  八 (CARDINAL)\n",
      "  五陵北原 (MONEY)\n",
      "  二十年 (DATE)\n",
      "  泉原 (PERSON)\n",
      "  王命 (PERSON)\n",
      "  洛陽人 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  十字作八 (DATE)\n",
      "  四 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  何處尋 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  日 (GPE)\n",
      "  日方 (GPE)\n",
      "  五 (CARDINAL)\n",
      "  資從 (DATE)\n",
      "  周 (PERSON)\n",
      "  今晨 (TIME)\n",
      "  見爾當 (PERSON)\n",
      "  冀 (PERSON)\n",
      "  何由熟 (PERSON)\n",
      "  離言說 (PERSON)\n",
      "  南夷謫 (GPE)\n",
      "  八月 (DATE)\n",
      "  平沙日 (GPE)\n",
      "  白骨亂 (PERSON)\n",
      "  胡窺 (PERSON)\n",
      "  何日平 (PERSON)\n",
      "  十四為 (CARDINAL)\n",
      "  千 (CARDINAL)\n",
      "  十五 (CARDINAL)\n",
      "  十六 (CARDINAL)\n",
      "  灔澦堆 (PERSON)\n",
      "  五月 (DATE)\n",
      "  一一生 (CARDINAL)\n",
      "  八月 (DATE)\n",
      "  西園草 (GPE)\n",
      "  三 (CARDINAL)\n",
      "  巴 (GPE)\n",
      "  十五 (CARDINAL)\n",
      "  三軍淚如雨 (DATE)\n",
      "  四月 (DATE)\n",
      "  南風 (GPE)\n",
      "  棗花 (PERSON)\n",
      "  日品 (NORP)\n",
      "  醉臥 (PERSON)\n",
      "  洛陽 (GPE)\n",
      "  昨日 (DATE)\n",
      "  四 (CARDINAL)\n",
      "  二 (CARDINAL)\n",
      "  千餘 (CARDINAL)\n",
      "  蔡女 (PERSON)\n",
      "  胡笳聲 (PERSON)\n",
      "  八 (CARDINAL)\n",
      "  四郊秋 (CARDINAL)\n",
      "  董夫子 (PERSON)\n",
      "  百鳥散 (CARDINAL)\n",
      "  陰且 (GPE)\n",
      "  斷絕 (GPE)\n",
      "  川為淨其波 (GPE)\n",
      "  南山截竹為 (ORG)\n",
      "  九雛鳴鳳亂 (EVENT)\n",
      "  百泉相 (PERCENT)\n",
      "  鹿門月 (PERSON)\n",
      "  松逕長 (GPE)\n",
      "  鶴樓 (GPE)\n",
      "  五岳 (PERSON)\n",
      "  九 (CARDINAL)\n",
      "  三 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  九 (CARDINAL)\n",
      "  為廬 (PERSON)\n",
      "  石鏡淸 (PERSON)\n",
      "  三 (CARDINAL)\n",
      "  九 (CARDINAL)\n",
      "  天台 (GPE)\n",
      "  四 (CARDINAL)\n",
      "  八千 (CARDINAL)\n",
      "  一夜 (TIME)\n",
      "  千 (CARDINAL)\n",
      "  林兮驚 (PERSON)\n",
      "  二 (CARDINAL)\n",
      "  點明作詩 (ORG)\n",
      "  吳姬 (GPE)\n",
      "  金陵子 (PERSON)\n",
      "  昨日 (DATE)\n",
      "  日 (GPE)\n",
      "  今日 (DATE)\n",
      "  世不稱意 (LOC)\n",
      "  川行 (PERSON)\n",
      "  九月 (DATE)\n",
      "  金山西見煙塵飛 (ORG)\n",
      "  金甲夜 (PERSON)\n",
      "  半夜 (TIME)\n",
      "  五花連 (CARDINAL)\n",
      "  草檄 (NORP)\n",
      "  金山西 (GPE)\n",
      "  四 (CARDINAL)\n",
      "  三 (CARDINAL)\n",
      "  二 (CARDINAL)\n",
      "  風急 (LOC)\n",
      "  石凍馬 (PERSON)\n",
      "  王甘苦 (PERSON)\n",
      "  辛 (PERSON)\n",
      "  勝古人 (DATE)\n",
      "  北風 (GPE)\n",
      "  白草折 (PERSON)\n",
      "  八月 (DATE)\n",
      "  一夜春 (TIME)\n",
      "  千樹萬 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  胡琴 (PERSON)\n",
      "  三十 (CARDINAL)\n",
      "  貴戚 (GPE)\n",
      "  二 (CARDINAL)\n",
      "  郭家 (PERSON)\n",
      "  二 (CARDINAL)\n",
      "  二 (CARDINAL)\n",
      "  七 (CARDINAL)\n",
      "  三 (CARDINAL)\n",
      "  柏裏 (GPE)\n",
      "  四 (CARDINAL)\n",
      "  文采風 (GPE)\n",
      "  四 (CARDINAL)\n",
      "  南薰殿 (GPE)\n",
      "  日牽來 (GPE)\n",
      "  九重眞 (DATE)\n",
      "  二 (CARDINAL)\n",
      "  玉花 (PERSON)\n",
      "  尊含 (PERSON)\n",
      "  弟子 (PERSON)\n",
      "  將軍畫善葢 (ORG)\n",
      "  佳士 (LOC)\n",
      "  四 (CARDINAL)\n",
      "  帷幄 (PERSON)\n",
      "  周南 (PERSON)\n",
      "  南極 (NORP)\n",
      "  柯如靑 (PERSON)\n",
      "  四十圍 (CARDINAL)\n",
      "  叅天 (GPE)\n",
      "  二千 (EVENT)\n",
      "  二 (CARDINAL)\n",
      "  揭明 (PERSON)\n",
      "  崔 (PERSON)\n",
      "  孔明廟 (PERSON)\n",
      "  莫怨嗟 (PERSON)\n",
      "  佳人公孫氏 (ORG)\n",
      "  與余 (GPE)\n",
      "  八千人 (CARDINAL)\n",
      "  第一 (ORDINAL)\n",
      "  五十年 (DATE)\n",
      "  姿映寒日 (LOC)\n",
      "  金粟堆 (PERSON)\n",
      "  瞿塘石 (PERSON)\n",
      "  石魚湖 (PERSON)\n",
      "  夏水欲 (PERSON)\n",
      "  水為沼 (LOC)\n",
      "  四 (CARDINAL)\n",
      "  佛畫 (GPE)\n",
      "  十圍 (CARDINAL)\n",
      "  風生衣 (PERSON)\n",
      "  二三子 (CARDINAL)\n",
      "  江陵 (GPE)\n",
      "  十生九死 (CARDINAL)\n",
      "  藏逃 (PERSON)\n",
      "  天路幽 (ORG)\n",
      "  一年明月 (DATE)\n",
      "  五 (CARDINAL)\n",
      "  四方 (GPE)\n",
      "  專其雄 (FAC)\n",
      "  絕頂 (NORP)\n",
      "  秋雨節 (PERSON)\n",
      "  王將 (PERSON)\n",
      "  周綱 (DATE)\n",
      "  四海沸 (CARDINAL)\n",
      "  揮天戈 (PERSON)\n",
      "  第一 (ORDINAL)\n",
      "  何處得紙本 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  孔子 (PERSON)\n",
      "  秦 (PRODUCT)\n",
      "  濯冠 (PERSON)\n",
      "  十 (CARDINAL)\n",
      "  百倍過 (PRODUCT)\n",
      "  大廈 (PERSON)\n",
      "  中 (GPE)\n",
      "  朝 (GPE)\n",
      "  日銷 (GPE)\n",
      "  六年 (DATE)\n",
      "  數紙 (GPE)\n",
      "  太平日 (FAC)\n",
      "  漁翁夜傍西巖宿 (ORG)\n",
      "  日出不見人 (TIME)\n",
      "  深閨人 (FAC)\n",
      "  百媚生 (PERCENT)\n",
      "  六 (CARDINAL)\n",
      "  温泉水 (PERSON)\n",
      "  芙蓉帳 (DATE)\n",
      "  春從 (CARDINAL)\n",
      "  三千人 (CARDINAL)\n",
      "  九重 (DATE)\n",
      "  千 (CARDINAL)\n",
      "  西出 (GPE)\n",
      "  六 (CARDINAL)\n",
      "  翠翹金 (GPE)\n",
      "  埃散 (PERSON)\n",
      "  泥土 (GPE)\n",
      "  八 (CARDINAL)\n",
      "  春風 (DATE)\n",
      "  李花開日 (PERSON)\n",
      "  阿監 (LOC)\n",
      "  八 (CARDINAL)\n",
      "  瓦冷霜 (PERSON)\n",
      "  王輾轉思 (PERSON)\n",
      "  王 (PERSON)\n",
      "  中 (GPE)\n",
      "  日 (GPE)\n",
      "  金合 (PERSON)\n",
      "  七月七日 (DATE)\n",
      "  夜半 (TIME)\n",
      "  舉酒欲 (PERSON)\n",
      "  千 (CARDINAL)\n",
      "  三 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  中無 (GPE)\n",
      "  六 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  京 (GPE)\n",
      "  第一 (ORDINAL)\n",
      "  五陵年少 (MONEY)\n",
      "  今年 (DATE)\n",
      "  明年 (DATE)\n",
      "  去年 (DATE)\n",
      "  江州 (PERSON)\n",
      "  中 (GPE)\n",
      "  朝 (GPE)\n",
      "  四 (CARDINAL)\n",
      "  五十載 (CARDINAL)\n",
      "  陰風 (GPE)\n",
      "  愬 (PERSON)\n",
      "  古 (GPE)\n",
      "  十四 (CARDINAL)\n",
      "  第一 (ORDINAL)\n",
      "  愈拜稽 (PERSON)\n",
      "  文成 (ORG)\n",
      "  三 (CARDINAL)\n",
      "  讒 (GPE)\n",
      "  今無 (DATE)\n",
      "  二 (CARDINAL)\n",
      "  胡騎 (PERSON)\n",
      "  身當恩 (PERSON)\n",
      "  北空 (FAC)\n",
      "  三 (CARDINAL)\n",
      "  寒聲一夜 (TIME)\n",
      "  李將軍 (PERSON)\n",
      "  胡雁 (PERSON)\n",
      "  胡兒眼 (PERSON)\n",
      "  十五 (CARDINAL)\n",
      "  金盤膾 (PERSON)\n",
      "  朱樓盡 (PERSON)\n",
      "  七香車 (QUANTITY)\n",
      "  九華帳 (DATE)\n",
      "  春牕曙滅 (ORG)\n",
      "  九 (CARDINAL)\n",
      "  九 (CARDINAL)\n",
      "  與〈 (PERSON)\n",
      "  李家 (PERSON)\n",
      "  十五二十 (CARDINAL)\n",
      "  胡馬騎 (PERSON)\n",
      "  三千 (CARDINAL)\n",
      "  李廣無 (PERSON)\n",
      "  今日 (DATE)\n",
      "  二 (CARDINAL)\n",
      "  賀蘭山 (PERSON)\n",
      "  明老 (DATE)\n",
      "  五 (CARDINAL)\n",
      "  雪色 (ORG)\n",
      "  寶劍 (PERSON)\n",
      "  千 (CARDINAL)\n",
      "  秦 (PERSON)\n",
      "  漁樵 (PERSON)\n",
      "  四 (CARDINAL)\n",
      "  八千歲 (CARDINAL)\n",
      "  西當 (GPE)\n",
      "  橫絕 (GPE)\n",
      "  六 (CARDINAL)\n",
      "  西遊何 (LOC)\n",
      "  胡為乎 (PERSON)\n",
      "  秋啼金 (PERSON)\n",
      "  憶君 (PERSON)\n",
      "  十千 (DATE)\n",
      "  四 (CARDINAL)\n",
      "  多歧路 (GPE)\n",
      "  今安 (DATE)\n",
      "  三百 (CARDINAL)\n",
      "  將進酒 (ORG)\n",
      "  與君 (GPE)\n",
      "  玉不足貴 (PERSON)\n",
      "  何為言 (PERSON)\n",
      "  五花馬 (CARDINAL)\n",
      "  與爾 (PERSON)\n",
      "  四十 (CARDINAL)\n",
      "  武皇開 (ORG)\n",
      "  二百州 (QUANTITY)\n",
      "  千 (CARDINAL)\n",
      "  三月三日天 (DATE)\n",
      "  雲幕 (GPE)\n",
      "  秦 (WORK_OF_ART)\n",
      "  四 (CARDINAL)\n",
      "  四 (CARDINAL)\n",
      "  何逡巡 (PERSON)\n",
      "  當軒 (ORG)\n",
      "  三 (CARDINAL)\n",
      "  江頭 (PERSON)\n",
      "  第一 (ORDINAL)\n",
      "  白馬嚼 (PERSON)\n",
      "  今何 (DATE)\n",
      "  胡騎塵 (PERSON)\n",
      "  欲往城 (LOC)\n",
      "  夜飛 (TIME)\n",
      "  九 (CARDINAL)\n",
      "  丁寧 (PERSON)\n",
      "  王孫泣 (PERSON)\n",
      "  日 (GPE)\n",
      "  王孫善 (PERSON)\n",
      "  北服南單于 (FAC)\n",
      "  王孫愼 (PERSON)\n",
      "  五陵佳 (DATE)\n",
      "\n",
      "==================================================\n",
      "HARVESTED WORD GROUPS FOR TEXT GENERATION\n",
      "==================================================\n",
      "--- NOUNS ---\n",
      "Total unique words: 2192\n",
      "['丈氷', '盛文史', '閑逸', '迢', '常存', '郜鼎', '趙瑟', '可度', '蒐于', '顔色', '為庶', '須行', '峯出', '重色', '大顙']\n",
      "\n",
      "\n",
      "--- VERBS ---\n",
      "Total unique words: 1653\n",
      "['登山', '帝照', '學', '當歌', '靑史', '致魂', '硯', '迴若', '轉', '蘭杜', '曰', '理枝', '柳春', '笑', '犬喧']\n",
      "\n",
      "\n",
      "--- ADJECTIVES ---\n",
      "Total unique words: 124\n",
      "['同居', '無', '氣象', '興壯', '猩', '相屬君', '幽居', '排空', '非巾', '門時', '無復', '陽度', '四面', '妥帖', '高樓']\n",
      "\n",
      "\n",
      "--- PROPER NOUNS ---\n",
      "Total unique words: 386\n",
      "['胡為乎', '點題', '李花開日', '久嘆嗟', '埃散', '蠶眠桑', '歌月', '雲', '廬山', '暮從', '九重眞', '林棲者', '碧絲', '黛色', '何處尋']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "\n",
    "text = \"\"\"蘭葉春葳蕤，桂華秋皎潔。欣欣此生意，自爾為佳節。誰知林棲者，聞風坐相悅。草木有本心，何求美人折。花詞以美人比君。江南有丹橘，經冬猶綠林。豈伊地氣暖，自有歲寒心。可以薦嘉客，奈何阻重深。運命唯所遇，循環不可尋。徒言樹桃李，此木豈無陰。暮從碧山下，山月隨人歸。却顧所來徑，蒼蒼橫翠微。四句下山。相擕及田家，童稚開荊扉。綠竹入幽徑，靑蘿拂行衣。四句過山人。歡言得所憇，宿。，美酒聊共揮。置酒。長歌吟松風，曲盡河星稀。我醉君復樂，陶然共㤀機花閒一壺酒，獨酌無相親。舉杯邀明月，對影成三人。月既不解飲，影徒隨我身。暫伴月將影，行樂須及春。我歌月徘徊，我舞影零亂。醒時回交歡，醉後各分散。永結無情遊，相期邈雲漢。燕草如碧絲，秦桑低綠枝。當君懷歸日，承「燕草」。是妾斷腸時承「秦桑」。。春風不相識，何事入羅幃。岱宗夫如何，齊魯青未了。造化鍾神秀，陰陽割昏曉。盪胸生曾雲，高。決眥入歸鳥。遠。會當凌絕頂，一覽衆山小。結明「望」字。人生不相見，動如參與商。今夕復何夕，共此燈燭光。少壯能幾時，鬢髮各已蒼。訪舊半為鬼，驚呼熱中腸。焉知二十載，重上君子堂。昔別君未婚，兒女忽成行。怡然敬父執，問我來何方。問答乃未已，兒女羅酒漿。夜雨剪春韭，新炊閒黃粱。主稱會面難，一舉累十觴。十觴亦不醉，感子故意長。明日隔山岳，世事兩𣴭𣴭。絶代有佳人，幽居在空谷。自云良家子，零落依草木。關中昔喪敗，兄弟遭殺戮。官高何足論，不得收骨肉。世情惡衰歇，萬事隨轉燭。夫壻輕薄兒，新人已如玉。已上叙佳人之遭遇。以下寫佳人之志節。合昏尚知時，鴛鴦不獨宿。但見新人笑，那聞舊人哭。在山泉水淸，出山泉水濁。侍婢賣珠迴，牽蘿補茅屋。摘花不插髮，采柏動盈掬。天寒翠袖薄，日暮倚修竹。死别已吞聲，生别常惻惻。江南瘴癘地，逐客無消息。故人入我夢，信其是。明我長相憶。恐非平生魂，路遠不可測。疑其非。餘亦有異。魂來楓林靑，又信其是。魂返關塞黑。君今在羅網，何以有羽翼？又疑其非。落月滿屋梁，猶疑照顏色。其來畢竟無疑。水深波浪闊，無使蛟龍得。其去恐有不測。浮雲終日行，遊子久不至。三夜頻夢君，情親見君意。告歸常局促，苦道來不易。六句夢中情景。江湖多風波，舟楫恐失墜。出門搔白首，若負平生志。冠蓋滿京華，斯人獨顦顇。六句醒後悲懷。孰云網恢恢，將老身反累。千秋萬歲名，寂寞身後事。下馬飲君酒，問君何所之。君言不得意，歸臥南山陲。但去莫復問，白雲無盡時。聖代無隱者，從赴試起。英靈盡來歸。遂令東山客，不得顧採薇。既至金門遠，四句落第。孰云吾道非。江淮渡寒食，京洛縫春衣。置酒長安道，四句還鄉。同心與我違。行當浮桂棹，未幾拂荊扉。遠樹帶行客，四句送行。孤城當落暉。吾謀適不用，勿謂知音稀。言入黃花川，每逐靑谿水。隨山將萬轉，趣途無百里。聲喧亂石中聞。，色靜深松裏。見。漾漾汎菱荇，澄澄映葭葦。溪上。我心素巳閑，淸川澹如此。請留盤石上，垂釣將巳矣。斜陽照墟落，窮巷牛羊歸。野老念牧童，倚杖候荊扉。雉雊麥苗秀，蠶眠桑葉稀。田夫荷鋤至，相見語依依。卽此羡閑逸，悵然吟式微。𧰚色天下重，西施𡨴久微。朝為越谿女，暮作吳宮𡚱。賤日豈殊衆？貴來方悟稀。邀人傅香粉，不自着羅衣。君寵益嬌態，君憐無是非。當時浣紗伴，莫得同車歸。持謝鄰家子，效顰安可希。北山白雲裏，蘭山。隱者自怡悅。相望試登高，登山。心隨雁飛滅。愁因薄暮起，興是淸秋發。秋。時見歸村人，沙行渡頭歇。下山。天邊樹若薺，遠望。江畔洲如月。近見。何當載酒來，寄張。共醉重陽節。山光忽西落，池月漸東上。散髮乘夕涼，夏日。開軒臥閑敞。南亭。荷風送香氣，氣。竹露滴淸響，聲。欲取鳴琴彈，恨無知音賞。懷辛。感此懷故人，中宵勞夢想。夕陽度西嶺，羣壑倏已暝。起宿意。松月生夜凉，見。風泉滿淸聽。聞。樵人歸欲盡，煙鳥棲初定。之子期宿來，孤琴候蘿徑。待丁不至。高臥南齋時，南齋。開帷月初吐。月。淸輝澹水木，演漾在窗戶。四月玩月。苒苒幾盈虛，澄澄變今古。美人淸江畔，憶崔。是夜越吟苦。山陰。千里其如何，微風吹蘭杜。絶頂一茅茨，直上三十里。扣關無僮僕，不遇。窺室唯案几。若非巾柴車，陸路。應是釣秋水。水路。差池不相見，黽俛空仰止。草色新雨中，見。松聲晚窗裏。聞。及茲契幽絶，自足蕩心耳！雖無賓主意，頗得淸淨理。興盡方下山，何必待之子。上山起，下山結。幽意無斷絶，此去隨所偶。晚風吹行舟，泛。花路入谿口。春際夜轉西壑，隔山望南斗。潭煙飛溶溶，林月低向後。生事且瀰漫，願為持竿叟。淸谿深不測，隱處惟孤雲。隱居。松際露微月，淸光猶為君。宿王。茅亭宿花影，藥院滋苔紋。余亦謝時去，西山鸞鶴羣。因宿起見。墖勢如湧出，先從下望上。孤高聳天宮。登臨出世界，磴道盤虛空。四句「登」。突兀壓神州，崢嶸如鬼工。四角礙白日，七層摩蒼穹。二句到頂。下窺指高鳥，俯聽聞驚風。連山若波濤，奔湊似朝東。八句四面之景。東。青槐夾馳道，宮館何玲瓏。南。秋色從西來，蒼然滿關中。西。五陵北原上，萬古靑濛濛。。北。淨理了可悟，勝因夙所宗。誓將挂冠去，覺道資無窮。昔年逢太平，山林二十年。泉原在庭戸，洞壑當門前。井稅有常期，日晏猶得眠。忽然遭世變，數歲親戎旃。今來典斯郡，山夷又紛然。城小賊不屠，人貧傷可憐。是以陷鄰境，此州獨見全。使臣將王命，豈不如賊焉！今彼徵歛者，廹之如火煎。誰能絶人命，以作時世賢。思欲委符節，引竿自刺船。將家就魚麥，歸老江湖邊。兵衞森畫戟，宴寢凝淸香。齋。海上風雨至，雨。逍遙池閣涼。煩痾近消散，嘉賓復滿堂。文士集。自慙居處崇，未瞻斯民康。理會是非遣，性達形迹忘。鮮肥屬時禁，燕。蔬果幸見嘗。俯飲一杯酒，仰聆金玉章。神歡體自輕，意欲凌風翔。吳中盛文史，羣彥今汪洋。方知大藩地，豈曰財賦強。悽悽去親愛，泛泛入烟霧。歸棹洛陽人，殘鐘廣陵樹。四句初發。十字作八層看。今朝為此別，何處還相遇。世事波上舟，㳂洄安得住。四句寄元。今朝郡齋冷，忽念山中客。澗底束荊薪，歸來煮白石。四句道士。欲持一瓢酒，遠慰風雨夕。落葉滿空山，何處尋行跡。四句寄。客從東方來，衣上灞陵雨。問客何為來，采山因買斧。㝠㝠花正開，颺颺燕新語。昨別今已春，鬢絲生幾縷？落帆逗淮鎭，停舫臨孤驛。浩浩風起波，聞。冥冥日沈夕。見。人歸山郭暗，陸路。雁下蘆洲白。水路。獨夜憶秦關，聽鐘未眠客。吏舍跼終年，處。出郭曠淸曙。楊柳散和風，近景。靑山澹吾慮。遠景。依叢適自憇，緣澗還復去。行。微雨靄芳原，見。春鳩鳴何處。聞。樂幽心屢止，心。遵事跡猶遽。物。終罷斯結廬，慕陶直可庶。永日方慽慽，出行復悠悠。女子今有行，大江泝輕舟。爾輩苦無恃，此五字一篇之主。撫念益慈柔。幼為長所育，兩別泣不休。對此結中腸，義往難復留。一筆收住。自小闕內訓，再伸前說。事姑貽我憂。頼茲托令門，仁䘏庶無尤。貧儉誠所尚，資從豈待周。孝恭遵婦道，容止順其猷。别離在今晨，見爾當何秋。居閒始自遣，臨感忽難收。歸來視幼女，零淚緣纓流。應前作收，歸到幼女。汲井潄寒齒，晨詣。淸心拂塵服。閒持貝葉書，步出東齋讀。讀經。眞源了無取，妄跡世所逐。遺言冀可冥，繕性何由熟。道人庭宇靜，苔色連深竹。超師院。日出霧露餘，靑松如膏沐。澹然離言說，悟悅心自足。久為簪組束，幸此南夷謫。閒依農圃鄰，偶似山林客。曉耕翻露草，夜榜響溪石。來往不逢人，長歌楚天碧。蟬鳴空桑林，八月蕭關道。出塞復入塞，處處黃蘆草。從來幽竹客，皆共塵沙老。莫學游俠兒，矜誇紫驑好。飲馬度秋水，水寒風似刀。平沙日未没，黯黯見臨洮。昔日長城戰，咸言意氣高。黃塵足今古，白骨亂蓬蒿。好大喜功，到頭總是黃塵白骨。明月出天山，月。山。蒼茫雲海間。長風幾萬里，吹度玉門關。關。漢下白登道，胡窺青海灣。由來征戰地，不見有人還。戍客望邊邑，思歸多苦顏。高樓當此夜，歎息未應閒。長安一片月，萬戸擣衣聲。秋風吹不盡，總是玉關情。何日平胡虜，良人罷遠征。妾髮初覆額，折花門前劇。郎騎竹馬來，繞床弄靑梅。同居長干里，兩小無嫌猜。十四為君婦，羞顏未嘗開。低頭向暗壁，千喚不一囘。十五始展眉，願同塵與灰。常存抱柱信！豈上望夫臺？十六君遠行，瞿塘灔澦堆。五月不可觸，猿聲天上哀。門前送行跡，一一生綠苔。苔深不能掃，落葉秋風早。八月蝴蝶黃，雙飛西園草。感此傷妾心，坐愁紅顏老。早晚下三巴，預將書報家。相迎不道遠，直至長風沙。梧桐相待老，鴛鴦會雙死。貞節貴殉夫，捨生亦如此。波瀾誓不起，妾心古井水。慈母手中線，遊子身上衣。臨行密密縫，意恐遲遲歸。誰言寸草心，報得三春暉。前不見古人，後不見來者。念天地之悠悠，獨愴然而涕下男兒事長征，少小幽燕客。賭勝馬蹄下，由來輕七尺。殺人莫敢前，鬚如蝟毛磔。黃雲隴底白雲飛，未得報恩不得歸。遼東小婦年十五，慣彈琵琶解歌舞。今為羗笛出塞聲，使我三軍淚如雨。四月南風大麥黃，出門時候。棗花未落桐葉長。靑山朝別暮還見，嘶馬出門思舊鄉。陳侯立身何坦蕩，陳平日品槩。虬鬚虎眉仍大顙。腹中貯書一萬卷。不肯低頭在草莽。東門酤酒飲我曹，心輕萬事如鴻毛。陳出門時意氣。醉臥不知白日暮，有時空望孤雲高。長河浪頭連天黑，津吏停舟渡不得。陳出路風波。鄭國遊人未及家，洛陽行子空歎息。聞道故林相識多，罷官昨日今如何？送別。主人有酒歡今夕，請奏鳴琴廣陵客。月照城頭烏半飛，用照。霜淒萬木風入衣。風冷。銅鑪華燭燭增輝，火以煖之。初彈〈淥水〉後〈楚妃〉。皆曲名。一聲已動物皆靜，四座無言星欲稀。二句寫旁聽者。淸淮奉使千餘里，敢告雲山從此始。蔡女昔造胡笳聲，敘胡笳來歷。一彈一十有八拍。胡人落淚沾邊草，漢使斷腸對歸客。古戍蒼蒼烽火寒，大荒陰𣲽飛雪白。先拂商絃後角、羽，四郊秋葉驚摵摵。董夫子，通神明，董大。深松竊聽來妖精，言遲更速皆應手，將往復旋如有情。空山百鳥散還合，萬里浮雲陰且晴。嘶酸雛雁失羣夜，斷絕胡兒戀母聲。川為淨其波，鳥亦罷其鳴。烏珠部落家鄉遠，邏娑沙塵哀怨生。幽音變調忽飄灑，長風吹林雨墮瓦。迸泉颯颯飛木末，野鹿呦呦走堂下。長安城連東掖垣，鳳凰池對靑瑣門。房給事。高才脫畧名與利，日夕望君抱琴至。南山截竹為觱篥，此樂本自龜茲出。先敘觱篥原委。流傳漢地曲轉竒，涼州胡人為我吹。安。吹傍隣聞者多歎息，遠客思鄉皆淚垂。世人解聽不解賞，以下寫觱篥聲中情景。長颷風中自來往。枯桑老柏寒颼飀，九雛鳴鳳亂啾啾。龍吟虎嘯一時發，萬籟百泉相與秋。忽然更作漁陽摻，黃雲蕭條白日暗。變調如聞楊柳春，上林繁花照眼新。歲夜高堂列明燭，美酒一杯聲一曲。山寺鳴鐘晝巳昏，漁梁渡頭爭渡喧。人隨沙岸向江村，余亦乘舟歸鹿門。鹿門月照開煙樹，忽到龎公棲隱處。巖扉松逕長寂寥，唯有幽人自來去。我本楚狂人，狂歌笑孔邱。手持綠玉杖，朝別黃鶴樓。五岳尋山不辭遠，一生好入名山遊。廬山秀出南斗傍，此叚自下望上。屏風九叠雲錦張。影落明湖青黛光，金闕前開二峰長。銀河倒掛三石梁，香鑪瀑布遙相望，迴崖沓嶂淩蒼蒼。翠影紅霞映朝日，鳥飛不到吳天長。登高北觀天地閒，四句自上臨下。大江茫茫去不還。黃雲萬里動風色，白波九道流雪山。好為廬山謠，興因廬山發。以下寄侍御。閒窺石鏡淸我心，謝公行處蒼苔沒。早服還丹無世情，琴心三叠道初成。遙見仙人彩雲裏，手把芙蓉朝玉京。先期汗漫九垓上，願接盧敖遊太淸。寄盧。海客談瀛洲，先作陪。烟濤微茫信難求。越人語天姥，雲霓明滅或可覩。天姥連天向天橫，敘天姥。勢拔五岳掩赤城。天台四萬八千丈，對此欲倒東南傾。我欲因之夢吳、越，入夢遊。一夜飛度鏡湖月。湖月照我影，送我至剡溪。謝公宿處今尚在，綠水蕩漾淸猿啼。腳著謝公屐，身登靑雲梯。半壁見海日，空中聞天雞。千巖萬壑路不定，迷花倚石忽已暝。熊咆龍吟殷巖泉，慄深林兮驚層巓。雲靑靑兮欲雨，水澹澹兮生煙。列缺霹𩆝，邱巒崩摧。洞天石扉，訇然中開。靑㝠浩蕩不見底，日月照耀金銀臺。霓為衣兮風為馬，雲之君兮紛紛而來下。虎鼓瑟兮鸞迴車，仙之人兮列如麻。忽魂悸以魄動，怳驚起而長嗟。惟覺時之枕席，失向來之煙霞。世間行樂亦如此，古來萬事東流水。二句結穴，點明作詩之旨。別君去時何時還，留別。且放白鹿靑崕閒。須行卽騎訪名山，安能摧眉折腰事權貴，使我不得開心顏。風吹柳花滿店香，吳姬壓酒勸客嘗。金陵子弟來相送，欲行不行各盡觴。請君試問東流水，別意與之誰短長棄我去者，昨日之日不可留；亂我心者，今日之日多煩憂。長風萬里送秋雁，對此可以酣高樓。蓬萊文章建安骨，校書。中間小謝又淸發。自喻。俱懷逸興壯思飛，欲上靑天覽日月。抽刀斷水水更流，舉杯消愁愁更愁。人生在世不稱意，明朝散髮弄扁舟。君不見，走馬川行雪海邊，平沙莽莽黃入天。川行形勢。輪臺九月風夜吼，一川碎石大如斗，隨風滿地石亂走。匈奴草黃馬正肥，金山西見煙塵飛。漢家大將西出師，出師西征。將軍金甲夜不脫。以下寫軍行之苦。半夜軍行戈相撥，風頭如刀面如割。馬毛帶雪汗氣蒸，五花連錢旋作氷，幕中草檄硯水凝。虜騎聞之應膽懾。料知短兵不敢接，軍師西門佇獻捷。輪臺城頭夜吹角，聞。輪臺城北旄頭落。見。羽書昨夜過渠黎：單于已在金山西。戍樓西望煙塵黑，漢兵屯在輪臺北。上將擁旄西出征，出師西征。平明吹笛大軍行。四邊伐鼓雪海湧，三軍大呼陰山動。二句所聞。虜塞兵氣連雲屯，戰塲白骨纏草根。劍河風急雲片濶，天寒。沙口石凍馬蹄脫。地凍。亞相勤王甘苦辛，送封。誓將報主靜邊塵。古來靑史誰不見，今見功名勝古人。北風捲地白草折，因風下雪。胡天八月卽飛雪。忽如一夜春風來，千樹萬樹梨花開。四句詠雪。散入珠簾濕羅幕，狐裘不煖錦衾薄。將軍角弓不得控，四句雪後之寒。都護鐵衣冷猶着。瀚海闌干百丈氷，因雪成氷。愁雲慘澹萬里凝。中軍置酒飲歸客，以下送武。胡琴琵琶與羗笛。紛紛暮雪下轅門，風掣紅旗凍不翻。輪臺東門送君去，去時雪滿天山路。山迴路轉不見君，雪上空留馬行處。仍歸到雪作結。國初已來畫鞍馬，神妙獨數江都王。將軍得名三十載，人間又見眞乘黃。曾貌先帝照夜白，先作陪襯。龍池十日飛霹靂。內府殷紅瑪腦盤，婕妤傳詔才人索。盌賜將軍拜舞歸，輕紈細綺相追飛。貴戚權門得筆跡，始覺屏障生光輝。昔日太眞拳毛騧，又二●。近時郭家獅子花。今之新圖有二馬，先二匹。復令識者久嘆嗟。此皆騎戰一敵萬，縞素漠漠開風沙。其餘七匹亦殊絕，又七匹。迴若寒空動煙雪。霜蹄蹴踏長楸間，馬官厮養紛成列。帶敘。可憐九馬爭神駿，總一筆。顧視淸高氣深穩。借問苦心愛者誰，後有韋諷前支遁。點韋。憶昔巡幸新豐宮，以下就馬發感慨。翠華拂天來向東。騰驤磊落三萬匹，皆與此圖筋骨同。自從獻寶朝河宗，無復射蛟江水中。君不見金粟堆前松柏裏，龍媒去盡鳥呼風。將軍魏武之子孫，四句敘曹家世。於今為庶為清門。英雄割據雖已矣，文采風流今尚存。學書初學衞夫人，四句以書●書。但恨無過王右軍。丹靑不知老將至，富貴於我如浮雲。開元之中常引見，承恩數上南薰殿。先寫畫人。凌煙功臣少顏色，將軍下筆開生面。良相頭上進賢冠，猛將腰間大羽箭。褒公鄂公毛髮動，英姿颯爽來酣戰。先帝天馬玉花驄，次寫畫馬。畫工如山貌不同。是日牽來赤墀下，迥立閶闔生長風。先寫真馬，只一句氣象萬千。詔謂將軍拂絹素，意匠慘淡經營中。斯須九重眞龍出，一洗萬古凡馬空。次寫畫馬，只二句已盡其工處。玉花𨚫在御榻上，榻上庭前屹相向。真馬、畫馬夾寫更奇！至尊含笑催賜金，圉人太僕皆惆悵。弟子韓幹早入室，餘波再敘。亦能畫馬窮殊相。幹惟畫肉不畫骨，忍使驊騮氣凋喪。收畫馬。言外見霸之工在畫骨。將軍畫善葢有神，必逢佳士亦寫眞。收畫人。卽今飄泊干戈際，屢貌尋常行路人。途窮反遭俗眼白，有欲節去此四句者，其說頗有見。世上未有如公貧。但看古來盛名下，終日坎𡒄𬘉其身。今我不樂思岳陽，身欲奮飛病在牀。美人娟娟隔秋水，濯足洞庭望八荒。鴻飛冥冥日月白，靑楓葉赤天雨霜。玉京羣帝集北斗，或騎麒麟翳鳳凰。芙蓉旌旗煙霧落，影動倒景搖瀟湘。星宮之君醉瓊漿，羽人稀少不在旁。似聞昨者赤松子，恐是漢代韓張良。昔隨劉氏定長安，帷幄未改神𢡖傷。國家成敗吾豈敢，色難腥腐餐楓香。周南留滯古所惜，南極老人應壽昌。美人胡為隔秋水，焉得置之見玉堂。結明詩旨。孔明廟前有老栢，柯如靑銅根如石。霜皮澑雨四十圍，黛色叅天二千尺。君臣已與時際會，樹木猶為人愛惜。二句揭明通首作意。雲來氣接巫峽長，月出寒通雪山白。憶昨路遶錦亭東，先主、武侯同閟宮。崔嵬枝幹郊原古，𥥆窕丹靑戸牖空。落落盤踞雖得地，㝠㝠孤高多烈風。扶持自是神明力，正直原因造化功。大厦如傾要梁棟，是古栢是孔明廟前之栢，正、喻夾發，言近指遠，託興遙深。萬牛迴首邱山重。不露文章世已驚，未辭剪伐誰能送。苦心豈免容螻蟻，香葉曾經宿鸞鳳。志士仁人莫怨嗟，古來材大難為用。結穴昔有佳人公孫氏，一舞劔器動四方。觀者如山色沮喪，天地為之久低昻。㸌如羿射九日落，矯如羣帝驂龍翔。來如雷霆收震怒，罷如江海凝淸光。絳唇珠袖兩寂寞，晚有弟子傳芬芳。臨潁美人在白帝，妙舞此曲神揚揚。與余問答旣有以，感時撫事增惋傷。先帝侍女八千人，公孫劔器初第一。五十年閒似反掌，風塵澒洞昬王室。梨園子弟散如煙，女樂餘姿映寒日。金粟堆前木已拱，瞿塘石城草蕭瑟。玳絃急管曲復終，樂極哀來月東出。老夫不知其所在，足繭荒山轉愁疾。石魚湖，似洞庭，夏水欲滿君山靑。山為樽，水為沼，酒徒歴歴坐洲島。長風連日作大浪，不能廢人運酒舫。我持長瓢坐巴邱，酌飲四座以散愁。山石犖确行徑微，黃昏到寺蝙蝠飛。升堂坐階新雨足，芭蕉葉大支子肥。僧言古壁佛畫好，以火來照所見稀，鋪床拂席置羹飯，疎糲亦足飽我飢。夜深靜臥百蟲絶，淸月出嶺光入扉。天明獨去無道路，出入高下窮烟霏。山紅澗碧紛爛熳，時見松櫪皆十圍。當流赤足踏澗石，水聲激激風生衣。人生如此自可樂，豈必侷促為人鞿。嗟哉吾黨二三子，安得至老不更歸？纖雲四卷天無河，淸風吹空月舒波。沙平水息聲影絶，一盃相屬君當歌。君歌聲酸辭正苦，不能聽終淚如雨。洞庭連天九疑高，此時公與張俱徙掾江陵，候命於郴而作。蛟龍出没猩鼯號。十生九死到官所，幽居黙黙如藏逃。下牀畏蛇食畏藥，海氣濕蟄薰腥臊。昨者州前搥大鼓，嗣皇繼聖登夔、臯。赦書一日行千里，罪從大辟皆除死。遷者追迴流者還，滌瑕蕩垢朝淸班。州家申名使家抑，坎軻秪得移荆蠻。判司卑官不堪說，未免捶楚塵埃間。同時輩流多大道，天路幽險難追攀。君歌且休聽我歌，我歌今與君殊科。一年明月今宵多，人生由命非由他，有酒不飲奈明何？五嶽祭秩皆三公，敘衡岳。四方環鎭嵩當中。火維地荒足妖怪，天假神柄專其雄。噴雲泄霧藏半腹，雖有絕頂誰能窮。我來正逢秋雨節，敘謁廟。陰氣晦昧無淸風。潛心黙禱若有應，豈非正直能感通？須㬰靜掃衆峯出，仰見突兀撐靑空。紫葢連延接天柱，石廩騰擲堆祝融。森然動魄下馬拜，松柏一逕趨靈宮。粉牆丹柱動光彩，鬼物圖畫塡靑紅。升階傴僂薦脯酒，欲以菲薄明其衷。廟令老人識神意，睢盱偵伺能鞠躬。手持盃珓導我擲，云此最古餘難同。竄逐蠻荒幸不死，衣食纔足甘長終。侯王將相望久絶，神縱欲福難為功。夜投佛寺上高閣，敘宿寺。星月掩映雲朣𬂔。猿鳴鐘動不知曙，杲杲寒日生於東。張生手持石鼓文，勸我試作石鼓歌。少陵無人謫仙死，才薄將奈石鼓何。周綱陵遲四海沸，先敘石鼓原委。宣王憤起揮天戈。大開明堂受朝賀，諸侯劔佩鳴相磨。蒐于岐陽騁雄俊，萬里禽獸皆遮羅。鐫功勒成告萬世，鑿石作鼓隳嵯峨。從臣才藝咸第一，揀選撰刻留山阿。雨淋日炙野火燎，鬼物守䕶煩撝呵。公從何處得紙本？毫髮盡備無差訛。辭嚴義密讀難曉，此段寫字體及文義之妙。字體不類隸與蝌。年深豈免有缺畫，快劍斫斷生蛟鼉。鸞翔鳳翥衆仙下，四句申明「字體」句。珊瑚碧樹交枝柯。金繩鐵索鎖鈕壯，古鼎躍水龍騰梭。陋儒編詩不收入，四句申明「辭嚴義密」句。二雅褊迫無委蛇。孔子西行不到秦，掎摭星宿遺羲娥。嗟余好古生苦晚，對此涕淚雙滂沱。憶昔初蒙博士徵，此段自述己見。其年始改稱元和。故人從軍在右輔，為我度量掘臼科。濯冠沐浴告祭酒，如此至寶存豈多。氊包席裹可立致，十鼓祗載數駱駝。薦諸太廟比郜鼎，襯筆。光價豈止百倍過。聖恩若許留太學，諸生講解得切磋。觀經鴻都尚塡咽，再襯。坐見舉國來奔波。剜苔剔蘚露節角，寄置妥帖平不頗。大廈深簷與葢覆，經歷久遠期無佗。中朝大官老於事，詎肯感激徒媕娿。牧童敲火牛礪角，此段嘆其失所。誰復著手為摩娑。日銷月鑠就埋沒，六年西顧空吟哦。羲之俗書趁姿媚，又作一襯。數紙尚可搏白鵞。繼周八代爭戰罷，無人收拾理則那。方今太平日無事，柄任儒術崇邱、軻。安能以此上論列，願借辨口如懸河。石鼓之歌止於此，嗚呼吾意其蹉跎。漁翁夜傍西巖宿，曉汲淸湘然楚竹。煙銷日出不見人，欸乃一聲山水綠。迴看天際下中流，巖上無心雲相逐。漢皇重色思傾國，御宇多年求不得。楊家有女初長成，養在深閨人未識。天生麗質難自棄，一朝選在君王側。回頭一笑百媚生，六宮粉黛無顏色。春寒賜浴華清池，温泉水滑洗凝脂。侍兒扶起嬌無力，始是新承恩澤時。雲𩯭花顔金步搖，芙蓉帳煖度春宵。春宵苦短日高起，從此君王不早朝。承歡侍宴無閒暇，春從春遊夜專夜。後宮佳麗三千人，三千寵愛在一身。金屋粧成嬌侍夜，玉樓宴罷醉和春。姊妹弟兄皆列土，可憐光彩生門戶。遂令天下父母心，不重生男重生女。驪宮高處入靑雲，仙樂風飄處處聞。緩歌謾舞凝絲竹，盡日君王看不足。漁陽鼙鼓動地來，驚破霓裳羽衣曲。九重城闕煙塵生，千乘萬騎西南行。翠華搖搖行復止，西出都門百餘里。六軍不發無奈何，宛轉蛾眉馬前死。花鈿委地無人收，翠翹金雀玉搔頭。君王掩面救不得，囘看血淚相和流。黃埃散漫風蕭索，雲棧縈紆登劍閣。峨媚山下少人行，旌旗無光日色薄。蜀江水碧蜀山靑，聖王朝朝暮暮情。行宮見月傷心色，夜雨聞鈴腸斷聲。天旋地轉迴龍馭，到此躊躇不能去。馬嵬坡下泥土中，不見玉顏空死處。君臣相顧盡霑衣，東望都門信馬歸。歸來池苑皆依舊，太液芙蓉未央柳。芙蓉如面柳如眉，以下八句寫目中情景，花草人物都到。對此如何不淚垂。春風桃李花開日，秋雨梧桐葉落時。西宮南內多秋草，落葉滿階紅不掃。梨園子弟白髮新，椒房阿監靑娥老。夕殿螢飛思悄然，以下八句寫夜閒情景，自初昏以至將曉都到。孤鐙挑盡未成眠。遲遲鐘鼓初長夜，耿耿星河欲曙天。鴛鴦瓦冷霜華重，翡翠衾寒誰與共。悠悠生死别經年，魂魄不曾來入夢。一句起下。臨卭道士鴻都客，能以精誠致魂魄。為感君王輾轉思，遂教方士殷勤覔。排空馭氣奔如電，升天入地求之徧。上窮碧落下黃泉，兩處茫茫皆不見。忽聞海內有仙山，山在虛無縹緲間。詼諧入妙。樓閣玲瓏五雲起，其中綽約多仙子。中有一人字太眞，雪膚花貌參差是。金闕西廂叩玉扃，轉敎小玉報雙成。聞道漢家天子使，九華帳裏夢魂驚。攬衣推枕起徘徊，珠箔銀屏迤邐開。雲髻半偏新睡覺，花冠不整下堂來。風吹仙袂飄飄舉，空虛處偏有實証。猶似霓裳羽衣舞。玉容寂寞淚闌干，梨花一枝春帶雨。含情凝睇謝君王，一别音容兩渺茫。昭陽殿裏恩愛絶，蓬萊宮中日月長。囘頭下望人寰處，不見長安見塵霧。惟將舊物表深情，鈿合金釵寄將去。釵留一股合一扇，釵擘黃金合分鈿。但教心似金鈿堅，天上人間會相見。臨别殷勤重寄詞，詞中有誓兩心知。七月七日長生殿，夜半無人私語時。在天願作比翼鳥，在地願為連理枝。天長地久有時盡，此恨綿綿無絶期。點題結穴。潯陽江頭夜送客，楓葉荻花秋瑟瑟。主人下馬客在船，舉酒欲飲無管絃。醉不成歡慘將别，别時茫茫江浸月。忽聞水上琵琶聲，主人忘歸客不發。尋聲闇問彈者誰，琵琶聲停欲語遲。移船相近邀相見，添酒擕燈重開宴。千呼萬喚始出來，猶抱琵琶半遮面。轉軸撥絃三兩聲，未成曲調先有情。絃絃掩抑聲聲思，四句為后文張本。似訴生平不得志。低眉信手續續彈，說盡心中無限事。輕攏慢撚抹復挑，以下寫琵琶。初為霓裳後六么。大絃嘈嘈如急雨，小絃切切如私語。嘈嘈切切錯雜彈，大珠小珠落玉盤。間關鶯語花底滑，幽咽流泉水下灘。水泉冷澀絃凝絶，凝絕不通聲漸歇。別有幽愁闇恨生，此時無聲勝有聲。銀瓶乍破水漿迸，鐵騎突出刀槍鳴。曲終收撥當心畫，四絃一聲如裂帛。東船西舫悄無言，唯見江心秋月白。應前。沈吟放撥插絃中，整頓衣裳起歛容。自言本是京城女，家在蝦蟆陵下住。十三學得琵琶成，名屬教坊第一部。曲罷常教善才服，妝成每被秋娘妬。五陵年少爭纒頭，一曲紅綃不知數。鈿頭銀篦擊節碎，血色羅裙翻酒污。今年歡笑復明年，秋月春風等閒度。弟走從軍阿姨死，暮去朝來顔色故。門前冷落車馬稀，老大嫁作商人婦。商人重利輕別離，前月浮梁買茶去。去來江口守空船，繞船明月江水寒。再應前。夜深忽夢少年事，夢啼妝淚紅闌干。我聞琵琶已歡息，又聞此語重喞喞。同是天涯淪落人，相逢何必曾相識。我從去年辭帝京，謫居臥病潯陽城。潯陽地僻無音樂，終歲不聞絲竹聲。住近湓城地低濕，黃蘆苦竹繞宅生。其間旦暮聞何物，杜鵑啼血猿哀鳴。春江花朝秋月夜，往往取酒還獨傾。豈無山歌與邨笛，嘔啞嘲哳難為聽。今夜聞君琵琶語，如聽仙樂耳暫明。莫辭更坐彈一曲，為君翻作琵琶行。感我此言良久立，却坐促絃絃轉急。淒凄不似向前聲，滿座重聞皆掩泣。座中泣者誰最多，江州司馬靑衫濕。元和天子神武姿，彼何人哉軒與羲。誓將上雪列聖耻，坐法宮中朝四夷。淮西有賊五十載，封狼生貙貙生羆。不據山河據平地，長戈利矛日可麾。帝得聖相相曰度，賊斫不死神扶持。腰懸相印作都統，陰風慘澹天王旗。愬、武、古、通作牙爪，儀曹外郎載筆隨。行軍司馬智且勇，十四萬衆猶虎𧴀。入蔡縛賊獻太廟，功無與讓恩不訾。帝曰汝度功第一，汝從事愈宜為辭。愈拜稽首蹈且舞，金石刻書臣能為。古者世稱大手筆，此事不係於職司。當仁自古有不讓，言訖屢頷天子頤。公退齋戒坐小閣，濡染大筆何淋漓。點竄堯典舜典字，詠韓碑即學韓體，才大者無所不可也。塗改淸廟生民詩。文成破體書在紙，淸晨再拜鋪丹墀。表曰臣愈昧死上，咏神聖功書之碑。碑高三丈字如斗，負以靈鼇蟠以螭。句竒語重喻者少，讒之天子言其私。長繩百尺拽碑倒，麄砂大石相磨治。公之斯文若元氣，先時巳入人肝脾。湯盤孔鼎有述作，今無其器存其辭。嗚呼聖王及聖相，相與烜赫流淳熙。公之斯文不示後，曷與三五相攀追。願書萬本誦萬遍，口角流沫右手胝。傳之七十有二代，以為封禪玉檢明堂基。漢家煙塵在東北，漢將辭家破殘賊。男兒本自重橫行，天子非常賜顏色。摐金伐鼓下榆關，旌旗逶迤碣石閒。校尉羽書飛瀚海，單于獵火照狼山。山川蕭條極邊土，路遠。胡騎憑淩雜風雨。敵勁。戰士軍前半死生，苦者自苦。美人帳下猶歌舞。樂者自樂。大漠窮秋塞草衰，邊塞。孤城落日鬪兵稀。兵少。身當恩遇常輕敵，本以身許。力盡關山未解圍。不克成功。鐵衣遠戍辛勤久，以下寫室家之思。玉筯應啼別離後。少婦城南欲斷腸，征人薊北空回首。邊風飄飄那可度，絶域蒼茫更何有。殺氣三時作陣雲，寒聲一夜傳刁斗。相看白刃血紛紛，死節從來豈顧勲。君不見沙場爭戰苦，至今猶憶李將軍。白日登山望烽火，昏黃飲馬傍交河。行人刁斗風砂暗，公主琵琶幽怨多。野營萬里無城郭，地廣。雨雪紛紛連大漠。天寒。胡雁哀鳴夜夜飛，所聞。胡兒眼淚雙雙落。所見。聞道玉門猶被遮，應將性命逐輕車。年年戰骨埋荒外，空見蒲萄入漢家。洛陽女兒對門居，纔可顏容十五餘。良人玉勒乘驄馬，侍女金盤膾鯉魚。畫閣朱樓盡相望，紅桃綠柳垂簷向。羅帷送上七香車，寶扇迎歸九華帳。狂夫富貴在靑春，意氣驕奢劇季倫。自憐碧玉親教舞，不惜珊瑚持與人。春牕曙滅九微火，九微片片飛花璅。戲罷曾無理曲時，與〈西施詠〉同一寓意。粧成秖是薰香坐。城中相識盡繁華，日夜經過趙李家。誰憐越女顏如玉，貧賤江頭自浣紗。少年十五二十時，步行奪得胡馬騎。射殺山中白額虎，肯數鄴下黃鬚兒？一身轉戰三千里，一劒曾當百萬師。漢兵奮迅如霹靂，虜騎崩騰畏蒺藜。衞靑不敗由天幸，李廣無功緣數奇。起下。自從棄置便衰朽，以下寫廢棄至老情景。世事蹉跎成白首。昔時飛箭無全目，今日垂楊生左肘。路旁時賣故侯瓜，門前學種先生柳。蒼茫古木連窮巷，寥落寒山對虛牖。誓令疎勒出飛泉，不似潁川空使酒。二句又起下。賀蘭山下陣如雲，以下明老而復起之故。羽檄交馳日夕聞。節使三河募年少，詔書五道出將軍。試拂鐵衣如雪色，聊持寶劍動星文。願得燕弓射大將，恥令越甲鳴吾君。莫嫌舊日雲中守，猶堪一戰立功勲。漁舟逐水愛山春，兩岸桃花夾古津。坐看紅樹不知遠，行盡靑溪忽值人。山口潛行始隈隩，山開曠望旋平陸。遙看一處攢雲樹，近入千家散花竹。樵客初傳漢姓名，居人未改秦衣服。居人共住武陵源，還從物外起田園。月明松下房櫳靜，日出雲中雞犬喧。驚聞俗客爭來集，競引還家問都邑。平明閭巷掃花開，薄暮漁樵乘水入。初因避地去人間，更問神仙遂不還。峽裏誰知有人事，世中遙望空雲山。不疑靈境難聞見，塵心未盡思鄕縣。出洞無論隔山水，辭家終擬長遊衍。自謂經過舊不迷，安知峰壑今來變。當時只記入山深，靑谿幾度到雲林。春來遍是桃花水，不辨仙源何處尋。噫吁嚱危乎高哉，蜀道之難、難於上靑天。蠶叢及魚鳬，開國何茫然。爾來四萬八千歲，乃與秦塞通人烟。西當太白有鳥道，可以橫絕峨眉顚，地崩山摧壯士死，然後天梯石棧方鈎連。上有六龍迴日之高標，下有衝波逆折之迴川。黃鶴之飛尚不得，黃鶴之飛尚不得過。猿猱欲度愁攀緣。靑泥何盤盤，百步九折縈巖巒。捫参歴井仰脅息，以手撫膺坐長歎。問君西遊何時還，畏途巉巖不可攀。但見悲鳥號古木，雄飛從雌繞林間。又聞子規啼夜月，愁空山。蜀道之難、難於上靑天。使人聽此彫朱顔。連峰去天不盈尺，枯松倒掛𠋣絶壁。飛湍瀑流爭喧豗，砯崖轉石萬壑雷。其嶮也若此！嗟爾遠道之人，胡為乎來哉？劔閣崢嶸而崔嵬，一夫當關，萬夫莫開。所守或匪親，化為狼與豺。朝避猛虎，夕避長蛇。磨牙吮血，殺人如麻。錦城雖云樂，不如早還家。結出通篇主意。蜀道之難、難於上靑天。側身西望長咨嗟。長相思，在長安。絡緯秋啼金井闌，微霜淒淒簟色寒。孤燈不明思欲絕，卷帷望月空長嘆，美人如花隔雲端。上有靑㝠之長天，下有綠水之波瀾。天長地遠魂飛苦，夢魂不到關山難。長相思，摧心肝。日色欲盡花含煙，月明欲素愁不眠。趙瑟初停鳳凰柱，蜀琴欲奏鴛鴦絃。此曲有意無人傳，願隨春風寄燕然。憶君迢迢隔靑天，昔時橫波目，今作流淚泉。不信妾腸斷，歸來看取明鏡前。金罇淸酒斗十千，玉盤珍羞直萬錢。停杯投筯不能食，拔劎四顧心茫然。欲渡黃河氷塞川，將登太行雪暗天。閒來垂釣坐溪上，忽復乘舟夢日邊。舉念不忘君。行路難，行路難，多歧路，今安在？長風破浪會有時，直挂雲帆濟滄海。君不見，黃河之水天上來，奔流到海不復回。君不見，高堂明鏡悲白髮，朝如靑絲暮成雪。人生得意須盡歡，莫使金樽空對月。此句一篇之主。天生我材必有用，千金散盡還復來。烹羊宰牛且為樂，會須一飲三百杯。岑夫子，丹邱生，將進酒，杯莫停。與君歌一曲，請君為我傾耳聽：鐘鼓饌玉不足貴，但願長醉不願醒。古來聖賢皆寂寞，唯有飲者留其名。陳王昔時宴平樂，斗酒十千恣歡謔。主人何為言少錢？徑須沽取對君酌。五花馬，千金裘，呼兒將出換美酒，與爾同銷萬古愁。車轔轔，馬蕭蕭，行人弓箭各在腰。耶娘妻子走相送，塵埃不見咸陽橋。牽衣頓足攔道哭，哭聲直上干雲霄。道傍過者問行人，行人但云點行頻。或從十五北防河，便至四十西營田。去時里正與裹頭，歸來頭白還戍邊。邊亭流血成海水，武皇開邊意未已。君不聞，漢家山東二百州，千村萬落生荆杞。縱有健婦把鋤犁，禾生隴畝無東西。況復秦兵耐苦戰，被驅不異犬與雞。長者雖有問，役夫敢申恨？且如今年冬，未休關西卒。縣官急索租，租稅從何出？信知生男惡，反是生女好。生女猶得嫁比隣，生男埋沒隨百草。君不見，靑海頭，古來白骨無人收。新鬼煩寃舊鬼哭，天陰雨濕聲啾啾。三月三日天氣新，長安水邊多麗人。態濃意遠淑且眞，神態。肌理細膩骨肉勻。繡羅衣裳照暮春，粧飾蹙金孔雀銀麒麟。頭上何所有，翠微㔩葉垂鬢脣。背後何所見，珠壓腰衱穩稱身。就中雲幕椒房親，以上泛咏麗人，此纔入秦、虢。賜名大國虢與秦。紫駝之峰出翠釜，四句寫其奢侈。水精之盤行素鱗。犀筯厭飫久未下，鸞刀縷切空紛綸。黃門飛鞚不動塵，四句寫其寵眷。御厨絡繹送八珍。簫鼓哀吟感鬼神，賓從雜遝實要津。後來鞍馬何逡巡，以下纔入國忠。當軒下馬入錦茵。楊花雪落覆白蘋，靑鳥飛去銜紅巾。炙手可𤍠勢絕倫，愼莫近前丞相嗔。少陵野老吞聲哭，三字通首眼目。春日潛行曲江曲。江頭宮殿鏁千門，細柳新蒲為誰綠？憶昔霓旌下南苑，苑中萬物生顔色。昭陽殿裏第一人，同輦隨君侍君側。輦前才人帶弓箭，白馬嚼齧黃金勒。翻身向天仰射雲，一箭正墜雙飛翼。明眸皓齒今何在？以下數語夫妻、父子死生離別，觸物引緒，字字俱有哭聲。血污遊魂歸不得，淸渭東流劍閣深，去、住彼此無消息。人生有情淚霑臆，江水江花豈終極。黃昏胡騎塵滿城，欲往城南望城北。長安城頭頭白烏，夜飛延秋門上呼。又向人家啄大屋，屋底達官走避胡。金鞭斷折九馬死，骨肉不待同馳驅。腰下寶玦靑珊瑚，先從寶玦看出，次從隆準看定，忠愛之心，倉卒之意，丁寧周至，如聞其聲。可憐王孫泣路隅。問之不肯道姓名，但道困苦乞為奴。已經百日竄荊棘，身上無有完肌膚。高帝子孫盡隆準，龍種自與常人殊。豺狼在邑龍在野，王孫善保千金軀。不敢長語臨郊衢，且為王孫立斯須。昨夜東風吹血腥，東來橐駝滿舊都。朔方健兒好身手，昔何勇銳今何愚。竊聞天子已傳位，聖德北服南單于。花門剺面請雪恥，愼勿出口他人狙。哀哉王孫愼勿疎，五陵佳氣無時無。\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "print(f\"\\nTotal sentences: {len(sentences)}\")\n",
    "print(\"\\nSample sentences:\")\n",
    "for sent in random.sample(sentences, min(3, len(sentences))):\n",
    "    print(f\"  - {sent.text.strip()}\")\n",
    "\n",
    "all_words = [token for token in doc if token.is_alpha]\n",
    "print(f\"\\nTotal words/characters extracted: {len(all_words)}\")\n",
    "print(\"\\nSample words:\")\n",
    "print([token.text for token in all_words[:20]])\n",
    "\n",
    "word_count = Counter([w.text for w in all_words])\n",
    "print(\"\\n\" + \"=\"*50) #only to visually divide the result, makes it easier readbable\n",
    "print(\"WORD FREQUENCY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nMost common 20 words:\")\n",
    "for word, count in word_count.most_common(20):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART OF SPEECH ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nouns = [token.text for token in all_words if token.pos_ == \"NOUN\"]\n",
    "print(f\"\\nNouns found ({len(nouns)} total):\")\n",
    "print(Counter(nouns).most_common(15))\n",
    "\n",
    "verbs = [token.text for token in all_words if token.pos_ == \"VERB\"]\n",
    "print(f\"\\nVerbs found ({len(verbs)} total):\")\n",
    "print(Counter(verbs).most_common(15))\n",
    "\n",
    "adjectives = [token.text for token in all_words if token.pos_ == \"ADJ\"]\n",
    "print(f\"\\nAdjectives found ({len(adjectives)} total):\")\n",
    "print(Counter(adjectives).most_common(15))\n",
    "\n",
    "proper_nouns = [token.text for token in all_words if token.pos_ == \"PROPN\"]\n",
    "print(f\"\\nProper nouns found ({len(proper_nouns)} total):\")\n",
    "print(Counter(proper_nouns).most_common(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NAMED ENTITIES\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNamed entities found in the poems:\")  # the results do not really make sense in the chinese spacy at least, as there are too many false positives which are actually just appeared as part of a sentence naturally\n",
    "for entity in doc.ents:\n",
    "    print(f\"  {entity.text} ({entity.label_})\")\n",
    "\n",
    "# there are no noun phrases in chinese spacy, so i removed that one\n",
    "#print(\"\\n\" + \"=\"*50)\n",
    "#print(\"NOUN PHRASES\")\n",
    "#print(\"=\"*50)\n",
    "# noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "#print(f\"\\nTotal noun phrases: {len(noun_phrases)}\")\n",
    "#print(\"\\nSample noun phrases:\")\n",
    "#for phrase in random.sample(noun_phrases, min(10, len(noun_phrases))):\n",
    "    #print(f\"  - {phrase}\")\n",
    "\n",
    "### I also skipped Lemmatization as it is not necessary in Chinese.\n",
    "#print(\"\\n\" + \"=\"*50)\n",
    "#print(\"LEMMATIZATION\")\n",
    "#print(\"=\"*50)\n",
    "#print(\"\\nVerbs in their basic form:\")\n",
    "#verb_lemmas = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "#print(Counter(verb_lemmas).most_common(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HARVESTED WORD GROUPS FOR TEXT GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# i use list/set here to remove duplication\n",
    "word_groups = {\n",
    "    \"nouns\": list(set(nouns)),\n",
    "    \"verbs\": list(set(verbs)),\n",
    "    \"adjectives\": list(set(adjectives)),\n",
    "    \"proper_nouns\": list(set(proper_nouns))\n",
    "    #\"noun_phrases\": list(set(noun_phrases))\n",
    "}\n",
    "\n",
    "print(\"--- NOUNS ---\")\n",
    "noun_list = word_groups[\"nouns\"] \n",
    "print(\"Total unique words:\", len(noun_list)) \n",
    "print(noun_list[0:15]) \n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- VERBS ---\")\n",
    "verb_list = word_groups[\"verbs\"]\n",
    "print(\"Total unique words:\", len(verb_list))\n",
    "print(verb_list[0:15])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- ADJECTIVES ---\")\n",
    "adj_list = word_groups[\"adjectives\"]\n",
    "print(\"Total unique words:\", len(adj_list))\n",
    "print(adj_list[0:15])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- PROPER NOUNS ---\")\n",
    "proper_list = word_groups[\"proper_nouns\"]\n",
    "print(\"Total unique words:\", len(proper_list))\n",
    "print(proper_list[0:15])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b614b9d-30a0-49c5-8058-9a744461cd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences: 1038\n",
      "Total words extracted: 6500\n",
      "\n",
      "==================================================\n",
      "TOP WORD FREQUENCIES\n",
      "==================================================\n",
      "不: 90\n",
      "之: 52\n",
      "一: 46\n",
      "有: 43\n",
      "句: 39\n",
      "我: 35\n",
      "在: 33\n",
      "四: 29\n",
      "此: 27\n",
      "去: 26\n",
      "上: 24\n",
      "未: 22\n",
      "見: 22\n",
      "如: 21\n",
      "已: 20\n",
      "\n",
      "==================================================\n",
      "POETIC VOCABULARY STATS\n",
      "==================================================\n",
      "Nouns (2-char): 1890 unique words\n",
      "Nouns (1-char): 245 unique words\n",
      "--------------------\n",
      "Verbs (2-char): 1207 unique words\n",
      "Verbs (1-char): 377 unique words\n",
      "--------------------\n",
      "Adjs  (2-char): 89 unique words\n",
      "Adjs  (1-char): 31 unique words\n",
      "\n",
      "==================================================\n",
      "GENERATED NONSENSE POEMS (based on 床前明月光 structure\n",
      "==================================================\n",
      "\n",
      "--- Poem #1 ---\n",
      "為用男埋高\n",
      "寶劍與雞紛\n",
      "相相槩鬚如\n",
      "不可眞絲竹\n",
      "\n",
      "--- Poem #2 ---\n",
      "煩痾閒暇樹\n",
      "成雪趙瑟眞\n",
      "龍翔歸綠枝\n",
      "秀出宰五月\n",
      "\n",
      "--- Poem #3 ---\n",
      "坦蕩為樂歲\n",
      "衰歇畫骨愬\n",
      "送封聖榆關\n",
      "流血完囘頭\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import random\n",
    "import tracery\n",
    "from tracery.modifiers import base_english\n",
    "\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "\n",
    "# text source: https://ctext.org/wiki.pl?if=gb&chapter=969585, i used google gemini for extracting the poems from the website: https://gemini.google.com/share/55f7d14e50d8\n",
    "# text = \"\"\"蘭葉春葳蕤，桂華秋皎潔。欣欣此生意，自爾為佳節。誰知林棲者，聞風坐相悅。草木有本心，何求美人折。花詞以美人比君。 暮從碧山下，山月隨人歸。却顧所來徑，蒼蒼橫翠微。四句下山。相擕及田家，童稚開荊扉。綠竹入幽徑，靑蘿拂行衣。四句過山人。歡言得所憇，宿。，美酒聊共揮。置酒。長歌吟松風，曲盡河星稀。我醉君復樂，陶然共㤀機 燕草如碧絲，秦桑低綠枝。當君懷歸日，承「燕草」。是妾斷腸時承「秦桑」。。春風不相識，何事入羅幃。 人生不相見，動如參與商。今夕復何夕，共此燈燭光。少壯能幾時，鬢髮各已蒼。訪舊半為鬼，驚呼熱中腸。焉知二十載，重上君子堂。昔別君未婚，兒女忽成行。怡然敬父執，問我來何方。問答乃未已，兒女羅酒漿。夜雨剪春韭，新炊閒黃粱。主稱會面難，一舉累十觴。十觴亦不醉，感子故意長。明日隔山岳，世事兩𣴭𣴭。 下馬飲君酒，問君何所之。君言不得意，歸臥南山陲。但去莫復問，白雲無盡時。 言入黃花川，每逐靑谿水。隨山將萬轉，趣途無百里。聲喧亂石中聞。，色靜深松裏。見。漾漾汎菱荇，澄澄映葭葦。溪上。我心素巳閑，淸川澹如此。請留盤石上，垂釣將巳矣。 斜陽照墟落，窮巷牛羊歸。野老念牧童，倚杖候荊扉。雉雊麥苗秀，蠶眠桑葉稀。田夫荷鋤至，相見語依依。卽此羡閑逸，悵然吟式微。 北山白雲裏，蘭山。隱者自怡悅。相望試登高，登山。心隨雁飛滅。愁因薄暮起，興是淸秋發。秋。時見歸村人，沙行渡頭歇。下山。天邊樹若薺，遠望。江畔洲如月。近見。何當載酒來，寄張。共醉重陽節。 山光忽西落，池月漸東上。散髮乘夕涼，夏日。開軒臥閑敞。南亭。荷風送香氣，氣。竹露滴淸響，聲。欲取鳴琴彈，恨無知音賞。懷辛。感此懷故人，中宵勞夢想。 幽意無斷絶，此去隨所偶。晚風吹行舟，泛。花路入谿口。春際夜轉西壑，隔山望南斗。潭煙飛溶溶，林月低向後。生事且瀰漫，願為持竿叟。 淸谿深不測，隱處惟孤雲。隱居。松際露微月，淸光猶為君。宿王。茅亭宿花影，藥院滋苔紋。余亦謝時去，西山鸞鶴羣。因宿起見。 悽悽去親愛，泛泛入烟霧。歸棹洛陽人，殘鐘廣陵樹。四句初發。十字作八層看。 今朝郡齋冷，忽念山中客。澗底束荊薪，歸來煮白石。四句道士。欲持一瓢酒，遠慰風雨夕。落葉滿空山，何處尋行跡。四句寄。 客從東方來，衣上灞陵雨。問客何為來，采山因買斧。㝠㝠花正開，颺颺燕新語。昨別今已春，鬢絲生幾縷？ 落帆逗淮鎭，停舫臨孤驛。浩浩風起波，聞。冥冥日沈夕。見。人歸山郭暗，陸路。雁下蘆洲白。水路。獨夜憶秦關，聽鐘未眠客。 汲井潄寒齒，晨詣。淸心拂塵服。閒持貝葉書，步出東齋讀。讀經。眞源了無取，妄跡世所逐。遺言冀可冥，繕性何由熟。道人庭宇靜，苔色連深竹。超師院。日出霧露餘，靑松如膏沐。澹然離言說，悟悅心自足。 久為簪組束，幸此南夷謫。閒依農圃鄰，偶似山林客。曉耕翻露草，夜榜響溪石。來往不逢人，長歌楚天碧。 飲馬度秋水，水寒風似刀。平沙日未没，黯黯見臨洮。昔日長城戰，咸言意氣高。黃塵足今古，白骨亂蓬蒿。好大喜功，到頭總是黃塵白骨。 明月出天山，月。山。蒼茫雲海間。長風幾萬里，吹度玉門關。關。漢下白登道，胡窺青海灣。由來征戰地，不見有人還。戍客望邊邑，思歸多苦顏。高樓當此夜，歎息未應閒。 妾髮初覆額，折花門前劇。郎騎竹馬來，繞床弄靑梅。同居長干里，兩小無嫌猜。十四為君婦，羞顏未嘗開。低頭向暗壁，千喚不一囘。十五始展眉，願同塵與灰。常存抱柱信！豈上望夫臺？十六君遠行，瞿塘灔澦堆。時明皇幸蜀，從行軍士久而未歸。五月不可觸，猿聲天上哀。門前送行跡，一一生綠苔。苔深不能掃，落葉秋風早。八月蝴蝶黃，雙飛西園草。感此傷妾心，坐愁紅顏老。早晚下三巴，預將書報家。相迎不道遠，直至長風沙。 梧桐相待老，鴛鴦會雙死。貞節貴殉夫，捨生亦如此。波瀾誓不起，妾心古井水。 慈母手中線，遊子身上衣。臨行密密縫，意恐遲遲歸。誰言寸草心，報得三春暉。\"\"\"\n",
    "text = \"\"\"蘭葉春葳蕤，桂華秋皎潔。欣欣此生意，自爾為佳節。誰知林棲者，聞風坐相悅。草木有本心，何求美人折。花詞以美人比君。江南有丹橘，經冬猶綠林。豈伊地氣暖，自有歲寒心。可以薦嘉客，奈何阻重深。運命唯所遇，循環不可尋。徒言樹桃李，此木豈無陰。暮從碧山下，山月隨人歸。却顧所來徑，蒼蒼橫翠微。四句下山。相擕及田家，童稚開荊扉。綠竹入幽徑，靑蘿拂行衣。四句過山人。歡言得所憇，宿。，美酒聊共揮。置酒。長歌吟松風，曲盡河星稀。我醉君復樂，陶然共㤀機花閒一壺酒，獨酌無相親。舉杯邀明月，對影成三人。月既不解飲，影徒隨我身。暫伴月將影，行樂須及春。我歌月徘徊，我舞影零亂。醒時回交歡，醉後各分散。永結無情遊，相期邈雲漢。燕草如碧絲，秦桑低綠枝。當君懷歸日，承「燕草」。是妾斷腸時承「秦桑」。。春風不相識，何事入羅幃。岱宗夫如何，齊魯青未了。造化鍾神秀，陰陽割昏曉。盪胸生曾雲，高。決眥入歸鳥。遠。會當凌絕頂，一覽衆山小。結明「望」字。人生不相見，動如參與商。今夕復何夕，共此燈燭光。少壯能幾時，鬢髮各已蒼。訪舊半為鬼，驚呼熱中腸。焉知二十載，重上君子堂。昔別君未婚，兒女忽成行。怡然敬父執，問我來何方。問答乃未已，兒女羅酒漿。夜雨剪春韭，新炊閒黃粱。主稱會面難，一舉累十觴。十觴亦不醉，感子故意長。明日隔山岳，世事兩𣴭𣴭。絶代有佳人，幽居在空谷。自云良家子，零落依草木。關中昔喪敗，兄弟遭殺戮。官高何足論，不得收骨肉。世情惡衰歇，萬事隨轉燭。夫壻輕薄兒，新人已如玉。已上叙佳人之遭遇。以下寫佳人之志節。合昏尚知時，鴛鴦不獨宿。但見新人笑，那聞舊人哭。在山泉水淸，出山泉水濁。侍婢賣珠迴，牽蘿補茅屋。摘花不插髮，采柏動盈掬。天寒翠袖薄，日暮倚修竹。死别已吞聲，生别常惻惻。江南瘴癘地，逐客無消息。故人入我夢，信其是。明我長相憶。恐非平生魂，路遠不可測。疑其非。餘亦有異。魂來楓林靑，又信其是。魂返關塞黑。君今在羅網，何以有羽翼？又疑其非。落月滿屋梁，猶疑照顏色。其來畢竟無疑。水深波浪闊，無使蛟龍得。其去恐有不測。浮雲終日行，遊子久不至。三夜頻夢君，情親見君意。告歸常局促，苦道來不易。六句夢中情景。江湖多風波，舟楫恐失墜。出門搔白首，若負平生志。冠蓋滿京華，斯人獨顦顇。六句醒後悲懷。孰云網恢恢，將老身反累。千秋萬歲名，寂寞身後事。下馬飲君酒，問君何所之。君言不得意，歸臥南山陲。但去莫復問，白雲無盡時。聖代無隱者，從赴試起。英靈盡來歸。遂令東山客，不得顧採薇。既至金門遠，四句落第。孰云吾道非。江淮渡寒食，京洛縫春衣。置酒長安道，四句還鄉。同心與我違。行當浮桂棹，未幾拂荊扉。遠樹帶行客，四句送行。孤城當落暉。吾謀適不用，勿謂知音稀。言入黃花川，每逐靑谿水。隨山將萬轉，趣途無百里。聲喧亂石中聞。，色靜深松裏。見。漾漾汎菱荇，澄澄映葭葦。溪上。我心素巳閑，淸川澹如此。請留盤石上，垂釣將巳矣。斜陽照墟落，窮巷牛羊歸。野老念牧童，倚杖候荊扉。雉雊麥苗秀，蠶眠桑葉稀。田夫荷鋤至，相見語依依。卽此羡閑逸，悵然吟式微。𧰚色天下重，西施𡨴久微。朝為越谿女，暮作吳宮𡚱。賤日豈殊衆？貴來方悟稀。邀人傅香粉，不自着羅衣。君寵益嬌態，君憐無是非。當時浣紗伴，莫得同車歸。持謝鄰家子，效顰安可希。北山白雲裏，蘭山。隱者自怡悅。相望試登高，登山。心隨雁飛滅。愁因薄暮起，興是淸秋發。秋。時見歸村人，沙行渡頭歇。下山。天邊樹若薺，遠望。江畔洲如月。近見。何當載酒來，寄張。共醉重陽節。山光忽西落，池月漸東上。散髮乘夕涼，夏日。開軒臥閑敞。南亭。荷風送香氣，氣。竹露滴淸響，聲。欲取鳴琴彈，恨無知音賞。懷辛。感此懷故人，中宵勞夢想。夕陽度西嶺，羣壑倏已暝。起宿意。松月生夜凉，見。風泉滿淸聽。聞。樵人歸欲盡，煙鳥棲初定。之子期宿來，孤琴候蘿徑。待丁不至。高臥南齋時，南齋。開帷月初吐。月。淸輝澹水木，演漾在窗戶。四月玩月。苒苒幾盈虛，澄澄變今古。美人淸江畔，憶崔。是夜越吟苦。山陰。千里其如何，微風吹蘭杜。絶頂一茅茨，直上三十里。扣關無僮僕，不遇。窺室唯案几。若非巾柴車，陸路。應是釣秋水。水路。差池不相見，黽俛空仰止。草色新雨中，見。松聲晚窗裏。聞。及茲契幽絶，自足蕩心耳！雖無賓主意，頗得淸淨理。興盡方下山，何必待之子。上山起，下山結。幽意無斷絶，此去隨所偶。晚風吹行舟，泛。花路入谿口。春際夜轉西壑，隔山望南斗。潭煙飛溶溶，林月低向後。生事且瀰漫，願為持竿叟。淸谿深不測，隱處惟孤雲。隱居。松際露微月，淸光猶為君。宿王。茅亭宿花影，藥院滋苔紋。余亦謝時去，西山鸞鶴羣。因宿起見。墖勢如湧出，先從下望上。孤高聳天宮。登臨出世界，磴道盤虛空。四句「登」。突兀壓神州，崢嶸如鬼工。四角礙白日，七層摩蒼穹。二句到頂。下窺指高鳥，俯聽聞驚風。連山若波濤，奔湊似朝東。八句四面之景。東。青槐夾馳道，宮館何玲瓏。南。秋色從西來，蒼然滿關中。西。五陵北原上，萬古靑濛濛。。北。淨理了可悟，勝因夙所宗。誓將挂冠去，覺道資無窮。昔年逢太平，山林二十年。泉原在庭戸，洞壑當門前。井稅有常期，日晏猶得眠。忽然遭世變，數歲親戎旃。今來典斯郡，山夷又紛然。城小賊不屠，人貧傷可憐。是以陷鄰境，此州獨見全。使臣將王命，豈不如賊焉！今彼徵歛者，廹之如火煎。誰能絶人命，以作時世賢。思欲委符節，引竿自刺船。將家就魚麥，歸老江湖邊。兵衞森畫戟，宴寢凝淸香。齋。海上風雨至，雨。逍遙池閣涼。煩痾近消散，嘉賓復滿堂。文士集。自慙居處崇，未瞻斯民康。理會是非遣，性達形迹忘。鮮肥屬時禁，燕。蔬果幸見嘗。俯飲一杯酒，仰聆金玉章。神歡體自輕，意欲凌風翔。吳中盛文史，羣彥今汪洋。方知大藩地，豈曰財賦強。悽悽去親愛，泛泛入烟霧。歸棹洛陽人，殘鐘廣陵樹。四句初發。十字作八層看。今朝為此別，何處還相遇。世事波上舟，㳂洄安得住。四句寄元。今朝郡齋冷，忽念山中客。澗底束荊薪，歸來煮白石。四句道士。欲持一瓢酒，遠慰風雨夕。落葉滿空山，何處尋行跡。四句寄。客從東方來，衣上灞陵雨。問客何為來，采山因買斧。㝠㝠花正開，颺颺燕新語。昨別今已春，鬢絲生幾縷？落帆逗淮鎭，停舫臨孤驛。浩浩風起波，聞。冥冥日沈夕。見。人歸山郭暗，陸路。雁下蘆洲白。水路。獨夜憶秦關，聽鐘未眠客。吏舍跼終年，處。出郭曠淸曙。楊柳散和風，近景。靑山澹吾慮。遠景。依叢適自憇，緣澗還復去。行。微雨靄芳原，見。春鳩鳴何處。聞。樂幽心屢止，心。遵事跡猶遽。物。終罷斯結廬，慕陶直可庶。永日方慽慽，出行復悠悠。女子今有行，大江泝輕舟。爾輩苦無恃，此五字一篇之主。撫念益慈柔。幼為長所育，兩別泣不休。對此結中腸，義往難復留。一筆收住。自小闕內訓，再伸前說。事姑貽我憂。頼茲托令門，仁䘏庶無尤。貧儉誠所尚，資從豈待周。孝恭遵婦道，容止順其猷。别離在今晨，見爾當何秋。居閒始自遣，臨感忽難收。歸來視幼女，零淚緣纓流。應前作收，歸到幼女。汲井潄寒齒，晨詣。淸心拂塵服。閒持貝葉書，步出東齋讀。讀經。眞源了無取，妄跡世所逐。遺言冀可冥，繕性何由熟。道人庭宇靜，苔色連深竹。超師院。日出霧露餘，靑松如膏沐。澹然離言說，悟悅心自足。久為簪組束，幸此南夷謫。閒依農圃鄰，偶似山林客。曉耕翻露草，夜榜響溪石。來往不逢人，長歌楚天碧。蟬鳴空桑林，八月蕭關道。出塞復入塞，處處黃蘆草。從來幽竹客，皆共塵沙老。莫學游俠兒，矜誇紫驑好。飲馬度秋水，水寒風似刀。平沙日未没，黯黯見臨洮。昔日長城戰，咸言意氣高。黃塵足今古，白骨亂蓬蒿。好大喜功，到頭總是黃塵白骨。明月出天山，月。山。蒼茫雲海間。長風幾萬里，吹度玉門關。關。漢下白登道，胡窺青海灣。由來征戰地，不見有人還。戍客望邊邑，思歸多苦顏。高樓當此夜，歎息未應閒。長安一片月，萬戸擣衣聲。秋風吹不盡，總是玉關情。何日平胡虜，良人罷遠征。妾髮初覆額，折花門前劇。郎騎竹馬來，繞床弄靑梅。同居長干里，兩小無嫌猜。十四為君婦，羞顏未嘗開。低頭向暗壁，千喚不一囘。十五始展眉，願同塵與灰。常存抱柱信！豈上望夫臺？十六君遠行，瞿塘灔澦堆。五月不可觸，猿聲天上哀。門前送行跡，一一生綠苔。苔深不能掃，落葉秋風早。八月蝴蝶黃，雙飛西園草。感此傷妾心，坐愁紅顏老。早晚下三巴，預將書報家。相迎不道遠，直至長風沙。梧桐相待老，鴛鴦會雙死。貞節貴殉夫，捨生亦如此。波瀾誓不起，妾心古井水。慈母手中線，遊子身上衣。臨行密密縫，意恐遲遲歸。誰言寸草心，報得三春暉。前不見古人，後不見來者。念天地之悠悠，獨愴然而涕下男兒事長征，少小幽燕客。賭勝馬蹄下，由來輕七尺。殺人莫敢前，鬚如蝟毛磔。黃雲隴底白雲飛，未得報恩不得歸。遼東小婦年十五，慣彈琵琶解歌舞。今為羗笛出塞聲，使我三軍淚如雨。四月南風大麥黃，出門時候。棗花未落桐葉長。靑山朝別暮還見，嘶馬出門思舊鄉。陳侯立身何坦蕩，陳平日品槩。虬鬚虎眉仍大顙。腹中貯書一萬卷。不肯低頭在草莽。東門酤酒飲我曹，心輕萬事如鴻毛。陳出門時意氣。醉臥不知白日暮，有時空望孤雲高。長河浪頭連天黑，津吏停舟渡不得。陳出路風波。鄭國遊人未及家，洛陽行子空歎息。聞道故林相識多，罷官昨日今如何？送別。主人有酒歡今夕，請奏鳴琴廣陵客。月照城頭烏半飛，用照。霜淒萬木風入衣。風冷。銅鑪華燭燭增輝，火以煖之。初彈〈淥水〉後〈楚妃〉。皆曲名。一聲已動物皆靜，四座無言星欲稀。二句寫旁聽者。淸淮奉使千餘里，敢告雲山從此始。蔡女昔造胡笳聲，敘胡笳來歷。一彈一十有八拍。胡人落淚沾邊草，漢使斷腸對歸客。古戍蒼蒼烽火寒，大荒陰𣲽飛雪白。先拂商絃後角、羽，四郊秋葉驚摵摵。董夫子，通神明，董大。深松竊聽來妖精，言遲更速皆應手，將往復旋如有情。空山百鳥散還合，萬里浮雲陰且晴。嘶酸雛雁失羣夜，斷絕胡兒戀母聲。川為淨其波，鳥亦罷其鳴。烏珠部落家鄉遠，邏娑沙塵哀怨生。幽音變調忽飄灑，長風吹林雨墮瓦。迸泉颯颯飛木末，野鹿呦呦走堂下。長安城連東掖垣，鳳凰池對靑瑣門。房給事。高才脫畧名與利，日夕望君抱琴至。南山截竹為觱篥，此樂本自龜茲出。先敘觱篥原委。流傳漢地曲轉竒，涼州胡人為我吹。安。吹傍隣聞者多歎息，遠客思鄉皆淚垂。世人解聽不解賞，以下寫觱篥聲中情景。長颷風中自來往。枯桑老柏寒颼飀，九雛鳴鳳亂啾啾。龍吟虎嘯一時發，萬籟百泉相與秋。忽然更作漁陽摻，黃雲蕭條白日暗。變調如聞楊柳春，上林繁花照眼新。歲夜高堂列明燭，美酒一杯聲一曲。山寺鳴鐘晝巳昏，漁梁渡頭爭渡喧。人隨沙岸向江村，余亦乘舟歸鹿門。鹿門月照開煙樹，忽到龎公棲隱處。巖扉松逕長寂寥，唯有幽人自來去。我本楚狂人，狂歌笑孔邱。手持綠玉杖，朝別黃鶴樓。五岳尋山不辭遠，一生好入名山遊。廬山秀出南斗傍，此叚自下望上。屏風九叠雲錦張。影落明湖青黛光，金闕前開二峰長。銀河倒掛三石梁，香鑪瀑布遙相望，迴崖沓嶂淩蒼蒼。翠影紅霞映朝日，鳥飛不到吳天長。登高北觀天地閒，四句自上臨下。大江茫茫去不還。黃雲萬里動風色，白波九道流雪山。好為廬山謠，興因廬山發。以下寄侍御。閒窺石鏡淸我心，謝公行處蒼苔沒。早服還丹無世情，琴心三叠道初成。遙見仙人彩雲裏，手把芙蓉朝玉京。先期汗漫九垓上，願接盧敖遊太淸。寄盧。海客談瀛洲，先作陪。烟濤微茫信難求。越人語天姥，雲霓明滅或可覩。天姥連天向天橫，敘天姥。勢拔五岳掩赤城。天台四萬八千丈，對此欲倒東南傾。我欲因之夢吳、越，入夢遊。一夜飛度鏡湖月。湖月照我影，送我至剡溪。謝公宿處今尚在，綠水蕩漾淸猿啼。腳著謝公屐，身登靑雲梯。半壁見海日，空中聞天雞。千巖萬壑路不定，迷花倚石忽已暝。熊咆龍吟殷巖泉，慄深林兮驚層巓。雲靑靑兮欲雨，水澹澹兮生煙。列缺霹𩆝，邱巒崩摧。洞天石扉，訇然中開。靑㝠浩蕩不見底，日月照耀金銀臺。霓為衣兮風為馬，雲之君兮紛紛而來下。虎鼓瑟兮鸞迴車，仙之人兮列如麻。忽魂悸以魄動，怳驚起而長嗟。惟覺時之枕席，失向來之煙霞。世間行樂亦如此，古來萬事東流水。二句結穴，點明作詩之旨。別君去時何時還，留別。且放白鹿靑崕閒。須行卽騎訪名山，安能摧眉折腰事權貴，使我不得開心顏。風吹柳花滿店香，吳姬壓酒勸客嘗。金陵子弟來相送，欲行不行各盡觴。請君試問東流水，別意與之誰短長棄我去者，昨日之日不可留；亂我心者，今日之日多煩憂。長風萬里送秋雁，對此可以酣高樓。蓬萊文章建安骨，校書。中間小謝又淸發。自喻。俱懷逸興壯思飛，欲上靑天覽日月。抽刀斷水水更流，舉杯消愁愁更愁。人生在世不稱意，明朝散髮弄扁舟。君不見，走馬川行雪海邊，平沙莽莽黃入天。川行形勢。輪臺九月風夜吼，一川碎石大如斗，隨風滿地石亂走。匈奴草黃馬正肥，金山西見煙塵飛。漢家大將西出師，出師西征。將軍金甲夜不脫。以下寫軍行之苦。半夜軍行戈相撥，風頭如刀面如割。馬毛帶雪汗氣蒸，五花連錢旋作氷，幕中草檄硯水凝。虜騎聞之應膽懾。料知短兵不敢接，軍師西門佇獻捷。輪臺城頭夜吹角，聞。輪臺城北旄頭落。見。羽書昨夜過渠黎：單于已在金山西。戍樓西望煙塵黑，漢兵屯在輪臺北。上將擁旄西出征，出師西征。平明吹笛大軍行。四邊伐鼓雪海湧，三軍大呼陰山動。二句所聞。虜塞兵氣連雲屯，戰塲白骨纏草根。劍河風急雲片濶，天寒。沙口石凍馬蹄脫。地凍。亞相勤王甘苦辛，送封。誓將報主靜邊塵。古來靑史誰不見，今見功名勝古人。北風捲地白草折，因風下雪。胡天八月卽飛雪。忽如一夜春風來，千樹萬樹梨花開。四句詠雪。散入珠簾濕羅幕，狐裘不煖錦衾薄。將軍角弓不得控，四句雪後之寒。都護鐵衣冷猶着。瀚海闌干百丈氷，因雪成氷。愁雲慘澹萬里凝。中軍置酒飲歸客，以下送武。胡琴琵琶與羗笛。紛紛暮雪下轅門，風掣紅旗凍不翻。輪臺東門送君去，去時雪滿天山路。山迴路轉不見君，雪上空留馬行處。仍歸到雪作結。國初已來畫鞍馬，神妙獨數江都王。將軍得名三十載，人間又見眞乘黃。曾貌先帝照夜白，先作陪襯。龍池十日飛霹靂。內府殷紅瑪腦盤，婕妤傳詔才人索。盌賜將軍拜舞歸，輕紈細綺相追飛。貴戚權門得筆跡，始覺屏障生光輝。昔日太眞拳毛騧，又二●。近時郭家獅子花。今之新圖有二馬，先二匹。復令識者久嘆嗟。此皆騎戰一敵萬，縞素漠漠開風沙。其餘七匹亦殊絕，又七匹。迴若寒空動煙雪。霜蹄蹴踏長楸間，馬官厮養紛成列。帶敘。可憐九馬爭神駿，總一筆。顧視淸高氣深穩。借問苦心愛者誰，後有韋諷前支遁。點韋。憶昔巡幸新豐宮，以下就馬發感慨。翠華拂天來向東。騰驤磊落三萬匹，皆與此圖筋骨同。自從獻寶朝河宗，無復射蛟江水中。君不見金粟堆前松柏裏，龍媒去盡鳥呼風。將軍魏武之子孫，四句敘曹家世。於今為庶為清門。英雄割據雖已矣，文采風流今尚存。學書初學衞夫人，四句以書●書。但恨無過王右軍。丹靑不知老將至，富貴於我如浮雲。開元之中常引見，承恩數上南薰殿。先寫畫人。凌煙功臣少顏色，將軍下筆開生面。良相頭上進賢冠，猛將腰間大羽箭。褒公鄂公毛髮動，英姿颯爽來酣戰。先帝天馬玉花驄，次寫畫馬。畫工如山貌不同。是日牽來赤墀下，迥立閶闔生長風。先寫真馬，只一句氣象萬千。詔謂將軍拂絹素，意匠慘淡經營中。斯須九重眞龍出，一洗萬古凡馬空。次寫畫馬，只二句已盡其工處。玉花𨚫在御榻上，榻上庭前屹相向。真馬、畫馬夾寫更奇！至尊含笑催賜金，圉人太僕皆惆悵。弟子韓幹早入室，餘波再敘。亦能畫馬窮殊相。幹惟畫肉不畫骨，忍使驊騮氣凋喪。收畫馬。言外見霸之工在畫骨。將軍畫善葢有神，必逢佳士亦寫眞。收畫人。卽今飄泊干戈際，屢貌尋常行路人。途窮反遭俗眼白，有欲節去此四句者，其說頗有見。世上未有如公貧。但看古來盛名下，終日坎𡒄𬘉其身。今我不樂思岳陽，身欲奮飛病在牀。美人娟娟隔秋水，濯足洞庭望八荒。鴻飛冥冥日月白，靑楓葉赤天雨霜。玉京羣帝集北斗，或騎麒麟翳鳳凰。芙蓉旌旗煙霧落，影動倒景搖瀟湘。星宮之君醉瓊漿，羽人稀少不在旁。似聞昨者赤松子，恐是漢代韓張良。昔隨劉氏定長安，帷幄未改神𢡖傷。國家成敗吾豈敢，色難腥腐餐楓香。周南留滯古所惜，南極老人應壽昌。美人胡為隔秋水，焉得置之見玉堂。結明詩旨。孔明廟前有老栢，柯如靑銅根如石。霜皮澑雨四十圍，黛色叅天二千尺。君臣已與時際會，樹木猶為人愛惜。二句揭明通首作意。雲來氣接巫峽長，月出寒通雪山白。憶昨路遶錦亭東，先主、武侯同閟宮。崔嵬枝幹郊原古，𥥆窕丹靑戸牖空。落落盤踞雖得地，㝠㝠孤高多烈風。扶持自是神明力，正直原因造化功。大厦如傾要梁棟，是古栢是孔明廟前之栢，正、喻夾發，言近指遠，託興遙深。萬牛迴首邱山重。不露文章世已驚，未辭剪伐誰能送。苦心豈免容螻蟻，香葉曾經宿鸞鳳。志士仁人莫怨嗟，古來材大難為用。結穴昔有佳人公孫氏，一舞劔器動四方。觀者如山色沮喪，天地為之久低昻。㸌如羿射九日落，矯如羣帝驂龍翔。來如雷霆收震怒，罷如江海凝淸光。絳唇珠袖兩寂寞，晚有弟子傳芬芳。臨潁美人在白帝，妙舞此曲神揚揚。與余問答旣有以，感時撫事增惋傷。先帝侍女八千人，公孫劔器初第一。五十年閒似反掌，風塵澒洞昬王室。梨園子弟散如煙，女樂餘姿映寒日。金粟堆前木已拱，瞿塘石城草蕭瑟。玳絃急管曲復終，樂極哀來月東出。老夫不知其所在，足繭荒山轉愁疾。石魚湖，似洞庭，夏水欲滿君山靑。山為樽，水為沼，酒徒歴歴坐洲島。長風連日作大浪，不能廢人運酒舫。我持長瓢坐巴邱，酌飲四座以散愁。山石犖确行徑微，黃昏到寺蝙蝠飛。升堂坐階新雨足，芭蕉葉大支子肥。僧言古壁佛畫好，以火來照所見稀，鋪床拂席置羹飯，疎糲亦足飽我飢。夜深靜臥百蟲絶，淸月出嶺光入扉。天明獨去無道路，出入高下窮烟霏。山紅澗碧紛爛熳，時見松櫪皆十圍。當流赤足踏澗石，水聲激激風生衣。人生如此自可樂，豈必侷促為人鞿。嗟哉吾黨二三子，安得至老不更歸？纖雲四卷天無河，淸風吹空月舒波。沙平水息聲影絶，一盃相屬君當歌。君歌聲酸辭正苦，不能聽終淚如雨。洞庭連天九疑高，此時公與張俱徙掾江陵，候命於郴而作。蛟龍出没猩鼯號。十生九死到官所，幽居黙黙如藏逃。下牀畏蛇食畏藥，海氣濕蟄薰腥臊。昨者州前搥大鼓，嗣皇繼聖登夔、臯。赦書一日行千里，罪從大辟皆除死。遷者追迴流者還，滌瑕蕩垢朝淸班。州家申名使家抑，坎軻秪得移荆蠻。判司卑官不堪說，未免捶楚塵埃間。同時輩流多大道，天路幽險難追攀。君歌且休聽我歌，我歌今與君殊科。一年明月今宵多，人生由命非由他，有酒不飲奈明何？五嶽祭秩皆三公，敘衡岳。四方環鎭嵩當中。火維地荒足妖怪，天假神柄專其雄。噴雲泄霧藏半腹，雖有絕頂誰能窮。我來正逢秋雨節，敘謁廟。陰氣晦昧無淸風。潛心黙禱若有應，豈非正直能感通？須㬰靜掃衆峯出，仰見突兀撐靑空。紫葢連延接天柱，石廩騰擲堆祝融。森然動魄下馬拜，松柏一逕趨靈宮。粉牆丹柱動光彩，鬼物圖畫塡靑紅。升階傴僂薦脯酒，欲以菲薄明其衷。廟令老人識神意，睢盱偵伺能鞠躬。手持盃珓導我擲，云此最古餘難同。竄逐蠻荒幸不死，衣食纔足甘長終。侯王將相望久絶，神縱欲福難為功。夜投佛寺上高閣，敘宿寺。星月掩映雲朣𬂔。猿鳴鐘動不知曙，杲杲寒日生於東。張生手持石鼓文，勸我試作石鼓歌。少陵無人謫仙死，才薄將奈石鼓何。周綱陵遲四海沸，先敘石鼓原委。宣王憤起揮天戈。大開明堂受朝賀，諸侯劔佩鳴相磨。蒐于岐陽騁雄俊，萬里禽獸皆遮羅。鐫功勒成告萬世，鑿石作鼓隳嵯峨。從臣才藝咸第一，揀選撰刻留山阿。雨淋日炙野火燎，鬼物守䕶煩撝呵。公從何處得紙本？毫髮盡備無差訛。辭嚴義密讀難曉，此段寫字體及文義之妙。字體不類隸與蝌。年深豈免有缺畫，快劍斫斷生蛟鼉。鸞翔鳳翥衆仙下，四句申明「字體」句。珊瑚碧樹交枝柯。金繩鐵索鎖鈕壯，古鼎躍水龍騰梭。陋儒編詩不收入，四句申明「辭嚴義密」句。二雅褊迫無委蛇。孔子西行不到秦，掎摭星宿遺羲娥。嗟余好古生苦晚，對此涕淚雙滂沱。憶昔初蒙博士徵，此段自述己見。其年始改稱元和。故人從軍在右輔，為我度量掘臼科。濯冠沐浴告祭酒，如此至寶存豈多。氊包席裹可立致，十鼓祗載數駱駝。薦諸太廟比郜鼎，襯筆。光價豈止百倍過。聖恩若許留太學，諸生講解得切磋。觀經鴻都尚塡咽，再襯。坐見舉國來奔波。剜苔剔蘚露節角，寄置妥帖平不頗。大廈深簷與葢覆，經歷久遠期無佗。中朝大官老於事，詎肯感激徒媕娿。牧童敲火牛礪角，此段嘆其失所。誰復著手為摩娑。日銷月鑠就埋沒，六年西顧空吟哦。羲之俗書趁姿媚，又作一襯。數紙尚可搏白鵞。繼周八代爭戰罷，無人收拾理則那。方今太平日無事，柄任儒術崇邱、軻。安能以此上論列，願借辨口如懸河。石鼓之歌止於此，嗚呼吾意其蹉跎。漁翁夜傍西巖宿，曉汲淸湘然楚竹。煙銷日出不見人，欸乃一聲山水綠。迴看天際下中流，巖上無心雲相逐。漢皇重色思傾國，御宇多年求不得。楊家有女初長成，養在深閨人未識。天生麗質難自棄，一朝選在君王側。回頭一笑百媚生，六宮粉黛無顏色。春寒賜浴華清池，温泉水滑洗凝脂。侍兒扶起嬌無力，始是新承恩澤時。雲𩯭花顔金步搖，芙蓉帳煖度春宵。春宵苦短日高起，從此君王不早朝。承歡侍宴無閒暇，春從春遊夜專夜。後宮佳麗三千人，三千寵愛在一身。金屋粧成嬌侍夜，玉樓宴罷醉和春。姊妹弟兄皆列土，可憐光彩生門戶。遂令天下父母心，不重生男重生女。驪宮高處入靑雲，仙樂風飄處處聞。緩歌謾舞凝絲竹，盡日君王看不足。漁陽鼙鼓動地來，驚破霓裳羽衣曲。九重城闕煙塵生，千乘萬騎西南行。翠華搖搖行復止，西出都門百餘里。六軍不發無奈何，宛轉蛾眉馬前死。花鈿委地無人收，翠翹金雀玉搔頭。君王掩面救不得，囘看血淚相和流。黃埃散漫風蕭索，雲棧縈紆登劍閣。峨媚山下少人行，旌旗無光日色薄。蜀江水碧蜀山靑，聖王朝朝暮暮情。行宮見月傷心色，夜雨聞鈴腸斷聲。天旋地轉迴龍馭，到此躊躇不能去。馬嵬坡下泥土中，不見玉顏空死處。君臣相顧盡霑衣，東望都門信馬歸。歸來池苑皆依舊，太液芙蓉未央柳。芙蓉如面柳如眉，以下八句寫目中情景，花草人物都到。對此如何不淚垂。春風桃李花開日，秋雨梧桐葉落時。西宮南內多秋草，落葉滿階紅不掃。梨園子弟白髮新，椒房阿監靑娥老。夕殿螢飛思悄然，以下八句寫夜閒情景，自初昏以至將曉都到。孤鐙挑盡未成眠。遲遲鐘鼓初長夜，耿耿星河欲曙天。鴛鴦瓦冷霜華重，翡翠衾寒誰與共。悠悠生死别經年，魂魄不曾來入夢。一句起下。臨卭道士鴻都客，能以精誠致魂魄。為感君王輾轉思，遂教方士殷勤覔。排空馭氣奔如電，升天入地求之徧。上窮碧落下黃泉，兩處茫茫皆不見。忽聞海內有仙山，山在虛無縹緲間。詼諧入妙。樓閣玲瓏五雲起，其中綽約多仙子。中有一人字太眞，雪膚花貌參差是。金闕西廂叩玉扃，轉敎小玉報雙成。聞道漢家天子使，九華帳裏夢魂驚。攬衣推枕起徘徊，珠箔銀屏迤邐開。雲髻半偏新睡覺，花冠不整下堂來。風吹仙袂飄飄舉，空虛處偏有實証。猶似霓裳羽衣舞。玉容寂寞淚闌干，梨花一枝春帶雨。含情凝睇謝君王，一别音容兩渺茫。昭陽殿裏恩愛絶，蓬萊宮中日月長。囘頭下望人寰處，不見長安見塵霧。惟將舊物表深情，鈿合金釵寄將去。釵留一股合一扇，釵擘黃金合分鈿。但教心似金鈿堅，天上人間會相見。臨别殷勤重寄詞，詞中有誓兩心知。七月七日長生殿，夜半無人私語時。在天願作比翼鳥，在地願為連理枝。天長地久有時盡，此恨綿綿無絶期。點題結穴。潯陽江頭夜送客，楓葉荻花秋瑟瑟。主人下馬客在船，舉酒欲飲無管絃。醉不成歡慘將别，别時茫茫江浸月。忽聞水上琵琶聲，主人忘歸客不發。尋聲闇問彈者誰，琵琶聲停欲語遲。移船相近邀相見，添酒擕燈重開宴。千呼萬喚始出來，猶抱琵琶半遮面。轉軸撥絃三兩聲，未成曲調先有情。絃絃掩抑聲聲思，四句為后文張本。似訴生平不得志。低眉信手續續彈，說盡心中無限事。輕攏慢撚抹復挑，以下寫琵琶。初為霓裳後六么。大絃嘈嘈如急雨，小絃切切如私語。嘈嘈切切錯雜彈，大珠小珠落玉盤。間關鶯語花底滑，幽咽流泉水下灘。水泉冷澀絃凝絶，凝絕不通聲漸歇。別有幽愁闇恨生，此時無聲勝有聲。銀瓶乍破水漿迸，鐵騎突出刀槍鳴。曲終收撥當心畫，四絃一聲如裂帛。東船西舫悄無言，唯見江心秋月白。應前。沈吟放撥插絃中，整頓衣裳起歛容。自言本是京城女，家在蝦蟆陵下住。十三學得琵琶成，名屬教坊第一部。曲罷常教善才服，妝成每被秋娘妬。五陵年少爭纒頭，一曲紅綃不知數。鈿頭銀篦擊節碎，血色羅裙翻酒污。今年歡笑復明年，秋月春風等閒度。弟走從軍阿姨死，暮去朝來顔色故。門前冷落車馬稀，老大嫁作商人婦。商人重利輕別離，前月浮梁買茶去。去來江口守空船，繞船明月江水寒。再應前。夜深忽夢少年事，夢啼妝淚紅闌干。我聞琵琶已歡息，又聞此語重喞喞。同是天涯淪落人，相逢何必曾相識。我從去年辭帝京，謫居臥病潯陽城。潯陽地僻無音樂，終歲不聞絲竹聲。住近湓城地低濕，黃蘆苦竹繞宅生。其間旦暮聞何物，杜鵑啼血猿哀鳴。春江花朝秋月夜，往往取酒還獨傾。豈無山歌與邨笛，嘔啞嘲哳難為聽。今夜聞君琵琶語，如聽仙樂耳暫明。莫辭更坐彈一曲，為君翻作琵琶行。感我此言良久立，却坐促絃絃轉急。淒凄不似向前聲，滿座重聞皆掩泣。座中泣者誰最多，江州司馬靑衫濕。元和天子神武姿，彼何人哉軒與羲。誓將上雪列聖耻，坐法宮中朝四夷。淮西有賊五十載，封狼生貙貙生羆。不據山河據平地，長戈利矛日可麾。帝得聖相相曰度，賊斫不死神扶持。腰懸相印作都統，陰風慘澹天王旗。愬、武、古、通作牙爪，儀曹外郎載筆隨。行軍司馬智且勇，十四萬衆猶虎𧴀。入蔡縛賊獻太廟，功無與讓恩不訾。帝曰汝度功第一，汝從事愈宜為辭。愈拜稽首蹈且舞，金石刻書臣能為。古者世稱大手筆，此事不係於職司。當仁自古有不讓，言訖屢頷天子頤。公退齋戒坐小閣，濡染大筆何淋漓。點竄堯典舜典字，詠韓碑即學韓體，才大者無所不可也。塗改淸廟生民詩。文成破體書在紙，淸晨再拜鋪丹墀。表曰臣愈昧死上，咏神聖功書之碑。碑高三丈字如斗，負以靈鼇蟠以螭。句竒語重喻者少，讒之天子言其私。長繩百尺拽碑倒，麄砂大石相磨治。公之斯文若元氣，先時巳入人肝脾。湯盤孔鼎有述作，今無其器存其辭。嗚呼聖王及聖相，相與烜赫流淳熙。公之斯文不示後，曷與三五相攀追。願書萬本誦萬遍，口角流沫右手胝。傳之七十有二代，以為封禪玉檢明堂基。漢家煙塵在東北，漢將辭家破殘賊。男兒本自重橫行，天子非常賜顏色。摐金伐鼓下榆關，旌旗逶迤碣石閒。校尉羽書飛瀚海，單于獵火照狼山。山川蕭條極邊土，路遠。胡騎憑淩雜風雨。敵勁。戰士軍前半死生，苦者自苦。美人帳下猶歌舞。樂者自樂。大漠窮秋塞草衰，邊塞。孤城落日鬪兵稀。兵少。身當恩遇常輕敵，本以身許。力盡關山未解圍。不克成功。鐵衣遠戍辛勤久，以下寫室家之思。玉筯應啼別離後。少婦城南欲斷腸，征人薊北空回首。邊風飄飄那可度，絶域蒼茫更何有。殺氣三時作陣雲，寒聲一夜傳刁斗。相看白刃血紛紛，死節從來豈顧勲。君不見沙場爭戰苦，至今猶憶李將軍。白日登山望烽火，昏黃飲馬傍交河。行人刁斗風砂暗，公主琵琶幽怨多。野營萬里無城郭，地廣。雨雪紛紛連大漠。天寒。胡雁哀鳴夜夜飛，所聞。胡兒眼淚雙雙落。所見。聞道玉門猶被遮，應將性命逐輕車。年年戰骨埋荒外，空見蒲萄入漢家。洛陽女兒對門居，纔可顏容十五餘。良人玉勒乘驄馬，侍女金盤膾鯉魚。畫閣朱樓盡相望，紅桃綠柳垂簷向。羅帷送上七香車，寶扇迎歸九華帳。狂夫富貴在靑春，意氣驕奢劇季倫。自憐碧玉親教舞，不惜珊瑚持與人。春牕曙滅九微火，九微片片飛花璅。戲罷曾無理曲時，與〈西施詠〉同一寓意。粧成秖是薰香坐。城中相識盡繁華，日夜經過趙李家。誰憐越女顏如玉，貧賤江頭自浣紗。少年十五二十時，步行奪得胡馬騎。射殺山中白額虎，肯數鄴下黃鬚兒？一身轉戰三千里，一劒曾當百萬師。漢兵奮迅如霹靂，虜騎崩騰畏蒺藜。衞靑不敗由天幸，李廣無功緣數奇。起下。自從棄置便衰朽，以下寫廢棄至老情景。世事蹉跎成白首。昔時飛箭無全目，今日垂楊生左肘。路旁時賣故侯瓜，門前學種先生柳。蒼茫古木連窮巷，寥落寒山對虛牖。誓令疎勒出飛泉，不似潁川空使酒。二句又起下。賀蘭山下陣如雲，以下明老而復起之故。羽檄交馳日夕聞。節使三河募年少，詔書五道出將軍。試拂鐵衣如雪色，聊持寶劍動星文。願得燕弓射大將，恥令越甲鳴吾君。莫嫌舊日雲中守，猶堪一戰立功勲。漁舟逐水愛山春，兩岸桃花夾古津。坐看紅樹不知遠，行盡靑溪忽值人。山口潛行始隈隩，山開曠望旋平陸。遙看一處攢雲樹，近入千家散花竹。樵客初傳漢姓名，居人未改秦衣服。居人共住武陵源，還從物外起田園。月明松下房櫳靜，日出雲中雞犬喧。驚聞俗客爭來集，競引還家問都邑。平明閭巷掃花開，薄暮漁樵乘水入。初因避地去人間，更問神仙遂不還。峽裏誰知有人事，世中遙望空雲山。不疑靈境難聞見，塵心未盡思鄕縣。出洞無論隔山水，辭家終擬長遊衍。自謂經過舊不迷，安知峰壑今來變。當時只記入山深，靑谿幾度到雲林。春來遍是桃花水，不辨仙源何處尋。噫吁嚱危乎高哉，蜀道之難、難於上靑天。蠶叢及魚鳬，開國何茫然。爾來四萬八千歲，乃與秦塞通人烟。西當太白有鳥道，可以橫絕峨眉顚，地崩山摧壯士死，然後天梯石棧方鈎連。上有六龍迴日之高標，下有衝波逆折之迴川。黃鶴之飛尚不得，黃鶴之飛尚不得過。猿猱欲度愁攀緣。靑泥何盤盤，百步九折縈巖巒。捫参歴井仰脅息，以手撫膺坐長歎。問君西遊何時還，畏途巉巖不可攀。但見悲鳥號古木，雄飛從雌繞林間。又聞子規啼夜月，愁空山。蜀道之難、難於上靑天。使人聽此彫朱顔。連峰去天不盈尺，枯松倒掛𠋣絶壁。飛湍瀑流爭喧豗，砯崖轉石萬壑雷。其嶮也若此！嗟爾遠道之人，胡為乎來哉？劔閣崢嶸而崔嵬，一夫當關，萬夫莫開。所守或匪親，化為狼與豺。朝避猛虎，夕避長蛇。磨牙吮血，殺人如麻。錦城雖云樂，不如早還家。結出通篇主意。蜀道之難、難於上靑天。側身西望長咨嗟。長相思，在長安。絡緯秋啼金井闌，微霜淒淒簟色寒。孤燈不明思欲絕，卷帷望月空長嘆，美人如花隔雲端。上有靑㝠之長天，下有綠水之波瀾。天長地遠魂飛苦，夢魂不到關山難。長相思，摧心肝。日色欲盡花含煙，月明欲素愁不眠。趙瑟初停鳳凰柱，蜀琴欲奏鴛鴦絃。此曲有意無人傳，願隨春風寄燕然。憶君迢迢隔靑天，昔時橫波目，今作流淚泉。不信妾腸斷，歸來看取明鏡前。金罇淸酒斗十千，玉盤珍羞直萬錢。停杯投筯不能食，拔劎四顧心茫然。欲渡黃河氷塞川，將登太行雪暗天。閒來垂釣坐溪上，忽復乘舟夢日邊。舉念不忘君。行路難，行路難，多歧路，今安在？長風破浪會有時，直挂雲帆濟滄海。君不見，黃河之水天上來，奔流到海不復回。君不見，高堂明鏡悲白髮，朝如靑絲暮成雪。人生得意須盡歡，莫使金樽空對月。此句一篇之主。天生我材必有用，千金散盡還復來。烹羊宰牛且為樂，會須一飲三百杯。岑夫子，丹邱生，將進酒，杯莫停。與君歌一曲，請君為我傾耳聽：鐘鼓饌玉不足貴，但願長醉不願醒。古來聖賢皆寂寞，唯有飲者留其名。陳王昔時宴平樂，斗酒十千恣歡謔。主人何為言少錢？徑須沽取對君酌。五花馬，千金裘，呼兒將出換美酒，與爾同銷萬古愁。車轔轔，馬蕭蕭，行人弓箭各在腰。耶娘妻子走相送，塵埃不見咸陽橋。牽衣頓足攔道哭，哭聲直上干雲霄。道傍過者問行人，行人但云點行頻。或從十五北防河，便至四十西營田。去時里正與裹頭，歸來頭白還戍邊。邊亭流血成海水，武皇開邊意未已。君不聞，漢家山東二百州，千村萬落生荆杞。縱有健婦把鋤犁，禾生隴畝無東西。況復秦兵耐苦戰，被驅不異犬與雞。長者雖有問，役夫敢申恨？且如今年冬，未休關西卒。縣官急索租，租稅從何出？信知生男惡，反是生女好。生女猶得嫁比隣，生男埋沒隨百草。君不見，靑海頭，古來白骨無人收。新鬼煩寃舊鬼哭，天陰雨濕聲啾啾。三月三日天氣新，長安水邊多麗人。態濃意遠淑且眞，神態。肌理細膩骨肉勻。繡羅衣裳照暮春，粧飾蹙金孔雀銀麒麟。頭上何所有，翠微㔩葉垂鬢脣。背後何所見，珠壓腰衱穩稱身。就中雲幕椒房親，以上泛咏麗人，此纔入秦、虢。賜名大國虢與秦。紫駝之峰出翠釜，四句寫其奢侈。水精之盤行素鱗。犀筯厭飫久未下，鸞刀縷切空紛綸。黃門飛鞚不動塵，四句寫其寵眷。御厨絡繹送八珍。簫鼓哀吟感鬼神，賓從雜遝實要津。後來鞍馬何逡巡，以下纔入國忠。當軒下馬入錦茵。楊花雪落覆白蘋，靑鳥飛去銜紅巾。炙手可𤍠勢絕倫，愼莫近前丞相嗔。少陵野老吞聲哭，三字通首眼目。春日潛行曲江曲。江頭宮殿鏁千門，細柳新蒲為誰綠？憶昔霓旌下南苑，苑中萬物生顔色。昭陽殿裏第一人，同輦隨君侍君側。輦前才人帶弓箭，白馬嚼齧黃金勒。翻身向天仰射雲，一箭正墜雙飛翼。明眸皓齒今何在？以下數語夫妻、父子死生離別，觸物引緒，字字俱有哭聲。血污遊魂歸不得，淸渭東流劍閣深，去、住彼此無消息。人生有情淚霑臆，江水江花豈終極。黃昏胡騎塵滿城，欲往城南望城北。長安城頭頭白烏，夜飛延秋門上呼。又向人家啄大屋，屋底達官走避胡。金鞭斷折九馬死，骨肉不待同馳驅。腰下寶玦靑珊瑚，先從寶玦看出，次從隆準看定，忠愛之心，倉卒之意，丁寧周至，如聞其聲。可憐王孫泣路隅。問之不肯道姓名，但道困苦乞為奴。已經百日竄荊棘，身上無有完肌膚。高帝子孫盡隆準，龍種自與常人殊。豺狼在邑龍在野，王孫善保千金軀。不敢長語臨郊衢，且為王孫立斯須。昨夜東風吹血腥，東來橐駝滿舊都。朔方健兒好身手，昔何勇銳今何愚。竊聞天子已傳位，聖德北服南單于。花門剺面請雪恥，愼勿出口他人狙。哀哉王孫愼勿疎，五陵佳氣無時無。\"\"\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# some basic analysis\n",
    "sentences = list(doc.sents)\n",
    "print(f\"\\nTotal sentences: {len(sentences)}\")\n",
    "\n",
    "all_words = [token for token in doc if token.is_alpha]\n",
    "print(f\"Total words extracted: {len(all_words)}\")\n",
    "\n",
    "word_count = Counter([w.text for w in all_words])\n",
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"TOP WORD FREQUENCIES\")\n",
    "print(\"=\"*50)\n",
    "for word, count in word_count.most_common(15):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# because spacy only filters by noun, verb and adjective, we have to filter additionally for the lenghts as we have to ensure it is strictly five characters\n",
    "\n",
    "nouns_2 = []  # 2-char nouns/proper nouns\n",
    "nouns_1 = []  # 1-char nouns/proper nouns\n",
    "verbs_2 = []  # 2-char verbs\n",
    "verbs_1 = []  # 1-char verbs\n",
    "adj_2 = []    # 2-char adjectives\n",
    "adj_1 = []    # 1-char adjectives\n",
    "\n",
    "for token in all_words:\n",
    "    word = token.text\n",
    "    length = len(word)\n",
    "    pos = token.pos_\n",
    "    \n",
    "    if pos == \"NOUN\" or pos == \"PROPN\":\n",
    "        if length == 2:\n",
    "            nouns_2.append(word)\n",
    "        elif length == 1:\n",
    "            nouns_1.append(word)\n",
    "           \n",
    "    elif pos == \"VERB\":\n",
    "        if length == 2:\n",
    "            verbs_2.append(word)\n",
    "        elif length == 1:\n",
    "            verbs_1.append(word)\n",
    "            \n",
    "    elif pos == \"ADJ\":\n",
    "        if length == 2:\n",
    "            adj_2.append(word)\n",
    "        elif length == 1:\n",
    "            adj_1.append(word)\n",
    "\n",
    "# i use list/set so i can remove any possible duplicates in our new little dictionary\n",
    "nouns_2 = list(set(nouns_2))\n",
    "nouns_1 = list(set(nouns_1))\n",
    "verbs_2 = list(set(verbs_2))\n",
    "verbs_1 = list(set(verbs_1))\n",
    "adj_2 = list(set(adj_2))\n",
    "adj_1 = list(set(adj_1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POETIC VOCABULARY STATS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Nouns (2-char): {len(nouns_2)} unique words\")\n",
    "print(f\"Nouns (1-char): {len(nouns_1)} unique words\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Verbs (2-char): {len(verbs_2)} unique words\")\n",
    "print(f\"Verbs (1-char): {len(verbs_1)} unique words\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Adjs  (2-char): {len(adj_2)} unique words\")\n",
    "print(f\"Adjs  (1-char): {len(adj_1)} unique words\")\n",
    "\n",
    "\n",
    "word_groups = {\n",
    "    \"nouns_2\": nouns_2,\n",
    "    \"nouns_1\": nouns_1,\n",
    "    \"verbs_2\": verbs_2,\n",
    "    \"verbs_1\": verbs_1,\n",
    "    \"adj_2\": adj_2,\n",
    "    \"adj_1\": adj_1\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED NONSENSE POEMS (based on 床前明月光 structure\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "rules = {\n",
    "    \"n2\": nouns_2,\n",
    "    \"n1\": nouns_1,\n",
    "    \"v2\": verbs_2,\n",
    "    \"v1\": verbs_1,\n",
    "    \"a2\": adj_2,\n",
    "    \"a1\": adj_1\n",
    "}\n",
    "\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Poem #{i+1} ---\")\n",
    "    \n",
    "    # Line 1 Structure: Noun(2) + Noun(2) + Adj(1)\n",
    "    # Example: 床前 (n2) + 明月 (n2) + 光 (a1)\n",
    "    print(grammar.flatten(\"#n2##n2##a1#\"))\n",
    "    \n",
    "    # Line 2 Structure: Verb(2) + Noun(2) + Noun(1)\n",
    "    # Example: 疑是 (v2) + 地上 (n2) + 霜 (n1)\n",
    "    print(grammar.flatten(\"#v2##n2##n1#\"))\n",
    "    \n",
    "    # Line 3 Structure: Verb(2) + Verb(1) + Noun(2)\n",
    "    # Example: 舉頭 (v2) + 望 (v1) + 明月 (n2)\n",
    "    print(grammar.flatten(\"#v2##v1##n2#\"))\n",
    "    \n",
    "    # Line 4 Structure: Verb(2) + Verb(1) + Noun(2)\n",
    "    # Example: 低頭 (v2) + 思 (v1) + 故鄉 (n2)\n",
    "    print(grammar.flatten(\"#v2##v1##n2#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714d1ba-d2c8-4b7d-a76d-187eea4698bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
